{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"assets/images/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Ensembles, Random Forests and Boosting\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "- Understand how and why decision trees can be improved using bagging and random forests.\n",
    "- Build random forest models for classification and regression.\n",
    "- Know how to extract the most important predictors in a random forest model.\n",
    "- Look at how to use weak learners (boosting) for classification\n",
    "- Tuning ML models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font style = 'color:blue'> What is Ensembling? </font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"assets/images/random_forest.png\" style=\"width:60%\" />\n",
    "\n",
    "Source: https://www.analyticsvidhya.com/blog/2020/05/decision-tree-vs-random-forest-algorithm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Ensemble learning (or \"ensembling\")** is the process of combining several predictive models in order to produce a combined model that is more accurate than any individual model. For example, given predictions from several models we could:\n",
    "\n",
    "- **Regression:** Take the average of the predictions.\n",
    "- **Classification:** Take a vote and use the most common prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For ensembling to work well, the models must be:\n",
    "\n",
    "- **Accurate:** They outperform the null model.\n",
    "- **Independent:** Their predictions are generated using different processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**The big idea:** If you have a collection of individually imperfect (and independent) models, the \"one-off\" mistakes made by each model are probably not going to be made by the rest of the models, and thus the mistakes will be discarded when you average the models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are two basic **methods for ensembling:**\n",
    "\n",
    "- Manually ensembling your individual models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Using a model that ensembles for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Manual Ensembling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Manual Ensemble Models](assets/images/ensemble_models.png)\n",
    "Source: https://towardsdatascience.com/ensemble-models-for-classification-d443ebed7efe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What makes an effective manual ensemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Different types of **models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Different combinations of **features**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Different **tuning parameters**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- (and it's how you win Kaggle competitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Machine learning flowchart](assets/images/crowdflower_ensembling.jpg)\n",
    "\n",
    "*Machine learning flowchart created by the [winner](https://github.com/ChenglongChen/Kaggle_CrowdFlower) of Kaggle's [CrowdFlower competition](https://www.kaggle.com/c/crowdflower-search-relevance)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparing Manual Ensembling With a Single Model Approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Advantages of manual ensembling:**\n",
    "\n",
    "- It increases predictive accuracy.\n",
    "- It's easy to get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Disadvantages of manual ensembling:**\n",
    "\n",
    "- It decreases interpretability.\n",
    "- It takes longer to train.\n",
    "- It takes longer to predict.\n",
    "- It is more complex to automate and maintain.\n",
    "- Small gains in accuracy may not be worth the added complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## <font style = 'color:blue'> Bagging </font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The primary weakness of **decision trees** is that they don't tend to have the best predictive accuracy. This is partially because of **high variance**, meaning that different splits in the training data can lead to very different trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Bagging** is a general-purpose procedure for reducing the variance of a machine learning method but is particularly useful for decision trees. Bagging is short for **bootstrap aggregation**, meaning the aggregation of bootstrap samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "A **bootstrap sample** is a random sample with replacement. So, it has the same size as the original sample but might duplicate some of the original observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "[ 6 12 13  9 10 12  6 16  1 17  2 13  8 14  7 19  6 19 12 11]\n"
     ]
    }
   ],
   "source": [
    "# Set a seed for reproducibility.\n",
    "np.random.seed(1)\n",
    "\n",
    "# Create an array of 1 through 20.\n",
    "nums = np.arange(1, 21)\n",
    "print(nums)\n",
    "\n",
    "# Sample that array 20 times with replacement.\n",
    "print(np.random.choice(a=nums, size=20, replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How does bagging work (for decision trees)?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Grow B trees using B bootstrap samples from the training data.\n",
    "2. Train each tree on its bootstrap sample and make predictions.\n",
    "3. Combine the predictions:\n",
    "    - Average the predictions for **regression trees**.\n",
    "    - Take a vote for **classification trees**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notes:\n",
    "\n",
    "- **Each bootstrap sample** should be the same size as the original training set. (It may contain repeated rows.)\n",
    "- **B** should be a large enough value that the error seems to have \"stabilized\".\n",
    "- The trees are **grown deep** so that they have low bias/high variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Bagging increases predictive accuracy by **reducing the variance**, similar to how cross-validation reduces the variance associated with train/test split (for estimating out-of-sample error) by splitting many times an averaging the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"manual-bagged\"></a>\n",
    "### Manually Implementing Bagged Decision Trees (with B=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year  miles  doors  vtype\n",
       "0  22000  2012  13000      2      0\n",
       "1  14000  2010  30000      2      0\n",
       "2  13000  2010  73500      4      0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'assets/data/vehicles_train.csv'\n",
    "train = pd.read_csv(path)\n",
    "train['vtype'] = train.vtype.map({'car':0, 'truck':1})\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([13,  2, 12,  2,  6,  1,  3, 10, 11,  9,  6,  1,  0,  1]),\n",
       " array([ 9,  0,  0,  9,  3, 13,  4,  0,  0,  4,  1,  7,  3,  2]),\n",
       " array([ 4,  7,  2,  4,  8, 13,  0,  7,  9,  3, 12, 12,  4,  6]),\n",
       " array([ 1,  5,  6, 11,  2,  1, 12,  8,  3, 10,  5,  0, 11,  2]),\n",
       " array([10, 10,  6, 13,  2,  4, 11, 11, 13, 12,  4,  6, 13,  3]),\n",
       " array([10,  0,  6,  4,  7, 11,  6,  7,  1, 11, 10,  5,  7,  9]),\n",
       " array([ 2,  4,  8,  1, 12,  2,  1,  1,  3, 12,  5,  9,  0,  8]),\n",
       " array([11,  1,  6,  3,  3, 11,  5,  9,  7,  9,  2,  3, 11,  3]),\n",
       " array([ 3,  8,  6,  9,  7,  6,  3,  9,  6, 12,  6, 11,  6,  1]),\n",
       " array([13, 10,  3,  4,  3,  1, 13,  0,  5,  8, 13,  6, 11,  8])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a seed for reproducibility.\n",
    "np.random.seed(123)\n",
    "\n",
    "# Create ten bootstrap samples (which will be used to select rows from the DataFrame).\n",
    "samples = [np.random.choice(a=14, size=14, replace=True) for _ in range(1, 11)]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors  vtype\n",
       "13   1300  1997  138000      4      0\n",
       "2   13000  2010   73500      4      0\n",
       "12   1800  1999  163000      2      1\n",
       "2   13000  2010   73500      4      0\n",
       "6    3000  2004  177000      4      0\n",
       "1   14000  2010   30000      2      0\n",
       "3    9500  2009   78000      4      0\n",
       "10   2500  2003  190000      2      1\n",
       "11   5000  2001   62000      4      0\n",
       "9    1900  2003  160000      4      0\n",
       "6    3000  2004  177000      4      0\n",
       "1   14000  2010   30000      2      0\n",
       "0   22000  2012   13000      2      0\n",
       "1   14000  2010   30000      2      0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the rows for the first decision tree.\n",
    "train.iloc[samples[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>130000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6000</td>\n",
       "      <td>2005</td>\n",
       "      <td>82500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12000</td>\n",
       "      <td>2010</td>\n",
       "      <td>60000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year   miles  doors  vtype\n",
       "0   3000  2003  130000      4      1\n",
       "1   6000  2005   82500      4      0\n",
       "2  12000  2010   60000      2      0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in and prepare the vehicle testing data.\n",
    "path = 'assets/data/vehicles_test.csv'\n",
    "test = pd.read_csv(path)\n",
    "test['vtype'] = test.vtype.map({'car':0, 'truck':1})\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Grow each tree deep.\n",
    "treereg = DecisionTreeRegressor(max_depth=None, random_state=123)\n",
    "\n",
    "# List for storing predicted price from each tree:\n",
    "predictions = []\n",
    "\n",
    "# Define testing data.\n",
    "X_test = test.iloc[:, 1:]\n",
    "y_test = test.iloc[:, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1300.,  5000., 14000.],\n",
       "       [ 1300.,  1300., 13000.],\n",
       "       [ 3000.,  3000., 13000.],\n",
       "       [ 4000.,  5000., 13000.],\n",
       "       [ 1300.,  5000., 13000.],\n",
       "       [ 4000.,  5000., 14000.],\n",
       "       [ 4000.,  4000., 13000.],\n",
       "       [ 4000.,  5000., 13000.],\n",
       "       [ 3000.,  5000.,  9500.],\n",
       "       [ 4000.,  5000.,  9000.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grow one tree for each bootstrap sample and make predictions on testing data.\n",
    "for sample in samples:\n",
    "    X_train = train.iloc[sample, 1:]\n",
    "    y_train = train.iloc[sample, 0]\n",
    "    treereg.fit(X_train, y_train)\n",
    "    y_pred = treereg.predict(X_test)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "# Convert predictions from list to NumPy array.\n",
    "predictions = np.array(predictions)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2990.,  4330., 12450.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average predictions.\n",
    "np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "998.5823284370031"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate RMSE.\n",
    "from sklearn import metrics\n",
    "y_pred = np.mean(predictions, axis=0)\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"manual-sklearn\"></a>\n",
    "### Bagged Decision Trees in `scikit-learn` (with B=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the training and testing sets.\n",
    "X_train = train.iloc[:, 1:]\n",
    "y_train = train.iloc[:, 0]\n",
    "X_test = test.iloc[:, 1:]\n",
    "y_test = test.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Instruct BaggingRegressor to use DecisionTreeRegressor as the \"base estimator.\"\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "bagreg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=500, bootstrap=True, oob_score=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3335. ,  5419.8, 12956. ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and predict.\n",
    "bagreg.fit(X_train, y_train)\n",
    "y_pred = bagreg.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673.9913550385247"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate RMSE.\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimating Out-of-Sample Error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For bagged models, out-of-sample error can be estimated without using **train/test split** or **cross-validation**!\n",
    "\n",
    "For each tree, the **unused observations** are called \"out-of-bag\" observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  2, 12,  2,  6,  1,  3, 10, 11,  9,  6,  1,  0,  1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first bootstrap sample.\n",
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 6, 9, 10, 11, 12, 13}\n",
      "{0, 1, 2, 3, 4, 7, 9, 13}\n",
      "{0, 2, 3, 4, 6, 7, 8, 9, 12, 13}\n",
      "{0, 1, 2, 3, 5, 6, 8, 10, 11, 12}\n",
      "{2, 3, 4, 6, 10, 11, 12, 13}\n",
      "{0, 1, 4, 5, 6, 7, 9, 10, 11}\n",
      "{0, 1, 2, 3, 4, 5, 8, 9, 12}\n",
      "{1, 2, 3, 5, 6, 7, 9, 11}\n",
      "{1, 3, 6, 7, 8, 9, 11, 12}\n",
      "{0, 1, 3, 4, 5, 6, 8, 10, 11, 13}\n"
     ]
    }
   ],
   "source": [
    "# Show the \"in-bag\" observations for each sample.\n",
    "for sample in samples:\n",
    "    print(set(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 7, 8]\n",
      "[5, 6, 8, 10, 11, 12]\n",
      "[1, 5, 10, 11]\n",
      "[4, 7, 9, 13]\n",
      "[0, 1, 5, 7, 8, 9]\n",
      "[2, 3, 8, 12, 13]\n",
      "[6, 7, 10, 11, 13]\n",
      "[0, 4, 8, 10, 12, 13]\n",
      "[0, 2, 4, 5, 10, 13]\n",
      "[2, 7, 9, 12]\n"
     ]
    }
   ],
   "source": [
    "# Show the \"out-of-bag\" observations for each sample.\n",
    "for sample in samples:\n",
    "    print(sorted(set(range(14)) - set(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Calculating \"out-of-bag error:\"**\n",
    "\n",
    "- For each observation in the training data, predict its response value using **only** the trees in which that observation was out-of-bag. Average those predictions (for regression) or take a vote (for classification).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Compare all predictions to the actual response values in order to compute the out-of-bag error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "When B is sufficiently large, the **out-of-bag error** is an accurate estimate of **out-of-sample error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7662607997982768"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the out-of-bag R-squared score (not MSE, unfortunately) for B=500.\n",
    "bagreg.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimating Feature Importance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Bagging increases **predictive accuracy** but decreases **model interpretability** because it's no longer possible to visualize the tree to understand the importance of each feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, we can still obtain an overall summary of **feature importance** from bagged models:\n",
    "\n",
    "- **Bagged regression trees:** Calculate the total amount that **MSE** decreases due to splits over a given feature, averaged over all trees\n",
    "- **Bagged classification trees:** Calculate the total amount that **Gini index** decreases due to splits over a given feature, averaged over all trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## <font style = 'color:blue'> Random Forests </font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Random Forest](assets/images/random_forest_il.png)\n",
    "Source: https://medium.com/rapids-ai/accelerating-random-forests-up-to-45x-using-cuml-dfb782a31bea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Random Forests offer a **slight variation on bagged trees** with even better performance:\n",
    "\n",
    "- Exactly like bagging, we create an ensemble of decision trees using bootstrapped samples of the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- However, when building each tree, each time a split is considered, a **random sample of m features** is chosen as split candidates from the **full set of p features**. The split is only allowed to use **one of those m features**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   - A new random sample of features is chosen for **every single tree at every single split**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   - For **classification**, m is typically chosen to be the square root of p.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   - For **regression**, m is typically chosen to be somewhere between p/3 and p.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What's the point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose there is **one very strong feature** in the data set. When using bagged trees, most of the trees will use that feature as the top split, resulting in an ensemble of similar trees that are **highly correlated**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Averaging highly correlated quantities does not significantly reduce variance (which is the entire goal of bagging)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By randomly leaving out candidate features from each split, **random forests \"decorrelate\" the trees** to the extent that the averaging process can reduce the variance of the resulting model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Another way of looking at it is that sometimes one or two strong features dominate every tree in bagging, resulting in essentially the same tree as every predictor. (This is what was meant when saying the trees could be highly correlated.) By using a subset of features to generate each tree, we get a wider variety of predictive trees that do not all use the same dominant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## <font style = 'color:blue'> Building and Tuning Decision Trees and Random Forests </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this section, we will implement random forests in scikit-learn.\n",
    "\n",
    "- Major League Baseball player data from 1986-87: [data](https://github.com/justmarkham/DAT8/blob/master/data/hitters.csv), [data dictionary](https://cran.r-project.org/web/packages/ISLR/ISLR.pdf) (page 7)\n",
    "- Each observation represents a player.\n",
    "- **Goal:** Predict player salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the data.\n",
    "path ='assets/data/hitters.csv'\n",
    "\n",
    "# Remove rows with missing values.\n",
    "hitters.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>594</td>\n",
       "      <td>169</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4408</td>\n",
       "      <td>1133</td>\n",
       "      <td>19</td>\n",
       "      <td>501</td>\n",
       "      <td>336</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>282</td>\n",
       "      <td>421</td>\n",
       "      <td>25</td>\n",
       "      <td>750.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "5    594   169      4    74   51     35     11    4408   1133      19    501   \n",
       "\n",
       "   CRBI  CWalks  League  Division  PutOuts  Assists  Errors  Salary  NewLeague  \n",
       "1   414     375       0         0      632       43      10   475.0          0  \n",
       "2   266     263       1         0      880       82      14   480.0          1  \n",
       "3   838     354       0         1      200       11       3   500.0          0  \n",
       "4    46      33       0         1      805       40       4    91.5          0  \n",
       "5   336     194       1         0      282      421      25   750.0          1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hitters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>594</td>\n",
       "      <td>169</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4408</td>\n",
       "      <td>1133</td>\n",
       "      <td>19</td>\n",
       "      <td>501</td>\n",
       "      <td>336</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>282</td>\n",
       "      <td>421</td>\n",
       "      <td>25</td>\n",
       "      <td>750.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "5    594   169      4    74   51     35     11    4408   1133      19    501   \n",
       "\n",
       "   CRBI  CWalks  League  Division  PutOuts  Assists  Errors  Salary  NewLeague  \n",
       "1   414     375       0         0      632       43      10   475.0          0  \n",
       "2   266     263       1         0      880       82      14   480.0          1  \n",
       "3   838     354       0         1      200       11       3   500.0          0  \n",
       "4    46      33       0         1      805       40       4    91.5          0  \n",
       "5   336     194       1         0      282      421      25   750.0          1  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode categorical variables as integers.\n",
    "hitters['League'] = pd.factorize(hitters.League)[0]\n",
    "hitters['Division'] = pd.factorize(hitters.Division)[0]\n",
    "hitters['NewLeague'] = pd.factorize(hitters.NewLeague)[0]\n",
    "hitters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Allow plots to appear in the notebook.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a scatter plot of hits vs years, colored by salary\n",
    "hitters.plot(kind='scatter', x='Years', y='Hits', c='Salary', colormap='jet', xlim=(0, 25), ylim=(0, 250));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', 'Walks', 'Years', 'League',\n",
       "       'Division', 'PutOuts', 'Assists', 'Errors', 'NewLeague'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define features: Exclude career statistics (which start with \"C\") and the response (salary).\n",
    "feature_cols = hitters.columns[hitters.columns.str.startswith('C') == False].drop('Salary')\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define X and y.\n",
    "X = hitters[feature_cols]\n",
    "y = hitters.Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Predicting Salary With a Decision Tree\n",
    "\n",
    "Let's first recall how we might predict salary using a single decision tree.\n",
    "\n",
    "We'll first find the best **max_depth** for a decision tree using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# List of values to try for max_depth:\n",
    "max_depth_range = list(range(1, 21))\n",
    "\n",
    "# List to store the average RMSE for each value of max_depth:\n",
    "RMSE_scores = []\n",
    "\n",
    "# Use 10-fold cross-validation with each value of max_depth.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "for depth in max_depth_range:\n",
    "    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
    "    MSE_scores = cross_val_score(treereg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot max_depth (x-axis) versus RMSE (y-axis).\n",
    "plt.plot(max_depth_range, RMSE_scores);\n",
    "plt.xlabel('max_depth');\n",
    "plt.ylabel('RMSE (lower is better)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(340.034168704752, 2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the best RMSE and the corresponding max_depth.\n",
    "sorted(zip(RMSE_scores, max_depth_range))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_depth=2 was best, so fit a tree using that parameter.\n",
    "treereg = DecisionTreeRegressor(max_depth=2, random_state=1)\n",
    "treereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hits</td>\n",
       "      <td>0.511609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Years</td>\n",
       "      <td>0.488391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AtBat</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HmRun</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Runs</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RBI</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Walks</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>League</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Division</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PutOuts</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Assists</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Errors</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NewLeague</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "1        Hits    0.511609\n",
       "6       Years    0.488391\n",
       "0       AtBat    0.000000\n",
       "2       HmRun    0.000000\n",
       "3        Runs    0.000000\n",
       "4         RBI    0.000000\n",
       "5       Walks    0.000000\n",
       "7      League    0.000000\n",
       "8    Division    0.000000\n",
       "9     PutOuts    0.000000\n",
       "10    Assists    0.000000\n",
       "11     Errors    0.000000\n",
       "12  NewLeague    0.000000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute feature importances.\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_}).sort_values(by='importance', \n",
    "                                                                                              ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"random-forest-demo\"></a>\n",
    "### Predicting Salary With a Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Fitting a Random Forest With the Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=5, n_estimators=150, oob_score=True,\n",
       "                      random_state=1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_features=5 is best and n_estimators=150 is sufficiently large.\n",
    "rfreg = RandomForestRegressor(n_estimators=150, max_features=5, oob_score=True, random_state=1)\n",
    "rfreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Years</td>\n",
       "      <td>0.224003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hits</td>\n",
       "      <td>0.144096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Walks</td>\n",
       "      <td>0.139355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RBI</td>\n",
       "      <td>0.133207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AtBat</td>\n",
       "      <td>0.093446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Runs</td>\n",
       "      <td>0.079611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PutOuts</td>\n",
       "      <td>0.062130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HmRun</td>\n",
       "      <td>0.041608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Errors</td>\n",
       "      <td>0.036901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Assists</td>\n",
       "      <td>0.028667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Division</td>\n",
       "      <td>0.008536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>League</td>\n",
       "      <td>0.004439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NewLeague</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "6       Years    0.224003\n",
       "1        Hits    0.144096\n",
       "5       Walks    0.139355\n",
       "4         RBI    0.133207\n",
       "0       AtBat    0.093446\n",
       "3        Runs    0.079611\n",
       "9     PutOuts    0.062130\n",
       "2       HmRun    0.041608\n",
       "11     Errors    0.036901\n",
       "10    Assists    0.028667\n",
       "8    Division    0.008536\n",
       "7      League    0.004439\n",
       "12  NewLeague    0.004000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute feature importances.\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':rfreg.feature_importances_}).sort_values(by='importance', \n",
    "                                                                                            ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5174330883603366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "295.47994138792376"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the out-of-bag R-squared score.\n",
    "print((rfreg.oob_score_))\n",
    "\n",
    "# Find the average RMSE.\n",
    "scores = cross_val_score(rfreg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Reducing X to its Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(263, 13)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of X.\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It important not to select features before separating your train from your test otherwise you are selecting features based on all known observations and introducing more of the information in the test data to the model when you fit it on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=5, n_estimators=150, oob_score=True,\n",
       "                      random_state=1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model on only the train data\n",
    "rfreg = RandomForestRegressor(n_estimators=150, max_features=5, oob_score=True, random_state=1)\n",
    "rfreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(197, 5)\n",
      "(197, 7)\n"
     ]
    }
   ],
   "source": [
    "# Set a threshold for which features to include.\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "print(SelectFromModel(rfreg, threshold='mean', prefit=True).transform(X_train).shape)\n",
    "print(SelectFromModel(rfreg, threshold='median', prefit=True).transform(X_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using the fit model and the features from the train data to transform the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new feature matrix that only includes important features.\n",
    "\n",
    "X_important =  SelectFromModel(rfreg, threshold='mean', prefit=True).transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314.39051968316204"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the RMSE for a random forest that only includes important features.\n",
    "rfreg = RandomForestRegressor(n_estimators=150, max_features=3, random_state=1)\n",
    "\n",
    "scores = cross_val_score(rfreg, X_important, y_test, cv=10, scoring='neg_mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this case, the error worsened slightly. Often parameter tuning is required to achieve optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Tuning Individual Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfreg = RandomForestRegressor()\n",
    "rfreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tuning n_estimators\n",
    "\n",
    "One important tuning parameter is **n_estimators**, which represents the number of trees that should be grown. This should be a large enough value that the error seems to have \"stabilized.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# List of values to try for n_estimators:\n",
    "estimator_range = list(range(10, 310, 10))\n",
    "\n",
    "# List to store the average RMSE for each value of n_estimators:\n",
    "RMSE_scores = []\n",
    "\n",
    "# Use five-fold cross-validation with each value of n_estimators (Warning: Slow!).\n",
    "for estimator in estimator_range:\n",
    "    rfreg = RandomForestRegressor(n_estimators=estimator, random_state=1)\n",
    "    MSE_scores = cross_val_score(rfreg, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot RMSE (y-axis) versus n_estimators (x-axis).\n",
    "\n",
    "plt.plot(estimator_range, RMSE_scores);\n",
    "\n",
    "plt.xlabel('n_estimators');\n",
    "plt.ylabel('RMSE (lower is better)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In theory, the RMSE will continue to decrease and eventually level out.  Adding more estimators will neither (noticably)increase or decrease the RMSE (or other loss metric). \n",
    "\n",
    "However, introduction of noise can lead to random spikes as the n_estimators changes. This example is particularly interesting as after about 120 estimators the RMSE seems to steadily rise as more estimators are added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tuning max_features\n",
    "\n",
    "The other important tuning parameter is **max_features**, which represents the number of features that should be considered at each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# List of values to try for max_features:\n",
    "feature_range = list(range(1, len(feature_cols)+1))\n",
    "\n",
    "# List to store the average RMSE for each value of max_features:\n",
    "RMSE_scores = []\n",
    "\n",
    "# Use 10-fold cross-validation with each value of max_features (Warning: Super slow!).\n",
    "for feature in feature_range:\n",
    "    rfreg = RandomForestRegressor(n_estimators=150, max_features=feature, random_state=1)\n",
    "    MSE_scores = cross_val_score(rfreg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot max_features (x-axis) versus RMSE (y-axis).\n",
    "\n",
    "plt.plot(feature_range, RMSE_scores);\n",
    "\n",
    "plt.xlabel('max_features');\n",
    "plt.ylabel('RMSE (lower is better)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289.1877447560242, 10)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the best RMSE and the corresponding max_features.\n",
    "sorted(zip(RMSE_scores, feature_range))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <font style = 'color:blue'> Comparing Random Forests With Decision Trees </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Advantages of random forests:**\n",
    "\n",
    "- Their performance is competitive with the best supervised learning methods.\n",
    "- They provide a more reliable estimate of feature importance.\n",
    "- They allow you to estimate out-of-sample error without using train/test split or cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Disadvantages of random forests:**\n",
    "\n",
    "- They are less interpretable.\n",
    "- They are slower to train.\n",
    "- They are slower to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font style = 'color:blue'> Boosting </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Boosted decision trees are weak learners (shallow trees) that are built sequentially.** \n",
    "\n",
    "This makes **Boosting the ideal algorithm to use when there is a large class imbalance** (e.g. Fraud detection). \n",
    "\n",
    "\n",
    "**After the first tree gets built, the algorithm analysis its performance and learns from it for the next tree.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## AdaBoost (adaptive boosting)\n",
    "\n",
    "In the case of AdaBoost (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) **misclassified observations get assigned higher weights.** Therefore the next weak learner will care more about the previously misclassified sample, than one it previously got right. The final prediction is a **weighted average** of the weak learners.\n",
    "\n",
    "![AdaBoost](assets/images/adaboost.png)\n",
    "\n",
    "Source: https://www.researchgate.net/figure/Illustration-of-AdaBoost-algorithm-for-creating-a-strong-classifier-based-on-multiple_fig9_288699540\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Gradient Boosting **does not modify the weights, but instead the wrongly predicted samples are used as input to the consecutive weak learner**, again giving more importance to the incorrectly classified samples. Again the final prediction is a **weighted average**.\n",
    "\n",
    "![Gradient Boosting](assets/images/gradient_boosting.png)\n",
    "\n",
    "Source :https://bradleyboehmke.github.io/HOML/gbm.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The 3 most popular Gradient Boosting algorithms (and the original papers describing them) are listed below:\n",
    "- XGBoost: https://xgboost.readthedocs.io/en/latest/ (https://arxiv.org/pdf/1603.02754.pdf)\n",
    "- (Microsoft's) LightGBM: https://lightgbm.readthedocs.io/en/latest/ (https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)\n",
    "- (Yandex's) CatBoost: https://catboost.ai/ (https://arxiv.org/pdf/1706.09516.pdf)\n",
    "\n",
    "**Not only are modern Boosting algorithms much quicker at building boosting trees than sklearn's `GradientBoostingClassifier`, but `LightGBM` and `CatBoost` also have inbuilt ways of dealing with categorical features so no `LabelEncoding` or `OneHotEncoding` has to be applied.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Predicting Titanic Survival with Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex   Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    1  22.0      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0  38.0      1      0   \n",
       "2                             Heikkinen, Miss. Laina    0  26.0      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0  35.0      1      0   \n",
       "4                           Allen, Mr. William Henry    1  35.0      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin Embarked  Embarked_Q  Embarked_S  \n",
       "0         A/5 21171   7.2500   NaN        S           0           1  \n",
       "1          PC 17599  71.2833   C85        C           0           0  \n",
       "2  STON/O2. 3101282   7.9250   NaN        S           0           1  \n",
       "3            113803  53.1000  C123        S           0           1  \n",
       "4            373450   8.0500   NaN        S           0           1  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data.\n",
    "path = 'assets/data/titanic.csv'\n",
    "titanic = pd.read_csv(path)\n",
    "\n",
    "# Encode female as 0 and male as 1.\n",
    "titanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\n",
    "\n",
    "# Fill in the missing values for age with the median age.\n",
    "titanic.Age.fillna(titanic.Age.median(), inplace=True)\n",
    "\n",
    "# Create a DataFrame of dummy variables for Embarked.\n",
    "embarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\n",
    "embarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Concatenate the original DataFrame and the dummy DataFrame.\n",
    "titanic = pd.concat([titanic, embarked_dummies], axis=1)\n",
    "\n",
    "# Print the updated DataFrame.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `sklearn.GradientBoostingClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch',\n",
       "       'Fare', 'Embarked_Q', 'Embarked_S'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting features by type \n",
    "titanic.select_dtypes(exclude=[\"object\", \"category\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    recall_score, \n",
    "    precision_score, \n",
    "    f1_score, \n",
    "    r2_score, \n",
    "    mean_squared_error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[163  19]\n",
      " [ 39  74]] \n",
      "Accuracy: 80.34 % \n",
      "Precision: 79.57 % \n",
      "Recall: 65.49 % \n",
      "F1: 71.84 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Define X and y.\n",
    "feature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S']\n",
    "\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, y_train)\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred),\n",
    "      \"\\nAccuracy:\", round(accuracy_score(y_test, y_pred) * 100, 2), \"%\",\n",
    "      \"\\nPrecision:\", round(precision_score(y_test, y_pred) * 100, 2), \"%\",\n",
    "      \"\\nRecall:\", round(recall_score(y_test, y_pred) * 100, 2), \"%\",      \n",
    "      \"\\nF1:\", round(f1_score(y_test, y_pred) * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Newer Boosting Algorighms\n",
    "- Have **inbuilt ways of dealing with categorical features**.\n",
    "- Are optimised (in the form of a few very clever tricks - read the papers for more details) to **train very quickly**.\n",
    "- Have **inbuild logic to use training and validation sets for model builds**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "titanic = titanic.dropna()\n",
    "\n",
    "feature_cols = [\"Pclass\", \"Sex\", \"Age\", \"Embarked\"]\n",
    "\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].astype('category')\n",
    "\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X_train, y_train, test_size=0.33, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "cat = [\"Embarked\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#!brew install libomp\n",
    "#!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-aa7c6b077b04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlgbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# dataset for lightgbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m train_set = lgbm.Dataset(data=X_train, \n\u001b[0;32m      5\u001b[0m                          \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgbm\n",
    "\n",
    "# dataset for lightgbm\n",
    "train_set = lgbm.Dataset(data=X_train, \n",
    "                         label=y_train,\n",
    "                         categorical_feature=cat, \n",
    "                         free_raw_data=False)\n",
    "\n",
    "validation_set = lgbm.Dataset(data=X_validation, \n",
    "                         label=y_validation,\n",
    "                         categorical_feature=cat, \n",
    "                         free_raw_data=False)\n",
    "\n",
    "test_set = lgbm.Dataset(data=X_test, \n",
    "                         label=y_test,\n",
    "                         categorical_feature=cat, \n",
    "                         free_raw_data=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# model parameters defined\n",
    "lgbm_params = {'max_depth': 3, \n",
    "               'learning_rate': 0.01, \n",
    "               'objective': 'binary', \n",
    "               'scale_pos_weight': y_train.value_counts()[0] / y_train.value_counts()[1] ,                    \n",
    "               'boost_from_average': False,\n",
    "               'feature_fraction' : 0.7,\n",
    "          }\n",
    "\n",
    "evaluation_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 60, number of negative: 30\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29\n",
      "[LightGBM] [Info] Number of data points in the train set: 90, number of used features: 3\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tTrain's binary_logloss: 0.688608\tValidate's binary_logloss: 0.689085\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tTrain's binary_logloss: 0.684164\tValidate's binary_logloss: 0.685118\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tTrain's binary_logloss: 0.682806\tValidate's binary_logloss: 0.684971\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tTrain's binary_logloss: 0.678472\tValidate's binary_logloss: 0.681112\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tTrain's binary_logloss: 0.677158\tValidate's binary_logloss: 0.680987\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tTrain's binary_logloss: 0.67293\tValidate's binary_logloss: 0.677232\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tTrain's binary_logloss: 0.671659\tValidate's binary_logloss: 0.677128\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tTrain's binary_logloss: 0.670412\tValidate's binary_logloss: 0.677038\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tTrain's binary_logloss: 0.666302\tValidate's binary_logloss: 0.673399\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tTrain's binary_logloss: 0.665096\tValidate's binary_logloss: 0.673328\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tTrain's binary_logloss: 0.663913\tValidate's binary_logloss: 0.67327\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tTrain's binary_logloss: 0.662755\tValidate's binary_logloss: 0.673225\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tTrain's binary_logloss: 0.661619\tValidate's binary_logloss: 0.673192\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tTrain's binary_logloss: 0.660506\tValidate's binary_logloss: 0.67317\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tTrain's binary_logloss: 0.659415\tValidate's binary_logloss: 0.67316\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tTrain's binary_logloss: 0.658345\tValidate's binary_logloss: 0.673161\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tTrain's binary_logloss: 0.657297\tValidate's binary_logloss: 0.673173\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tTrain's binary_logloss: 0.656269\tValidate's binary_logloss: 0.673196\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tTrain's binary_logloss: 0.655262\tValidate's binary_logloss: 0.673228\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tTrain's binary_logloss: 0.654274\tValidate's binary_logloss: 0.67327\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\tTrain's binary_logloss: 0.653306\tValidate's binary_logloss: 0.673322\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tTrain's binary_logloss: 0.652357\tValidate's binary_logloss: 0.673383\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tTrain's binary_logloss: 0.651426\tValidate's binary_logloss: 0.673453\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\tTrain's binary_logloss: 0.647576\tValidate's binary_logloss: 0.670061\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\tTrain's binary_logloss: 0.646675\tValidate's binary_logloss: 0.670142\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\tTrain's binary_logloss: 0.645792\tValidate's binary_logloss: 0.670233\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tTrain's binary_logloss: 0.64204\tValidate's binary_logloss: 0.666936\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tTrain's binary_logloss: 0.641185\tValidate's binary_logloss: 0.667037\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\tTrain's binary_logloss: 0.640347\tValidate's binary_logloss: 0.667145\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tTrain's binary_logloss: 0.639525\tValidate's binary_logloss: 0.667261\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tTrain's binary_logloss: 0.638719\tValidate's binary_logloss: 0.667384\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\tTrain's binary_logloss: 0.637928\tValidate's binary_logloss: 0.667514\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\tTrain's binary_logloss: 0.637153\tValidate's binary_logloss: 0.667651\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\tTrain's binary_logloss: 0.636393\tValidate's binary_logloss: 0.667794\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tTrain's binary_logloss: 0.632781\tValidate's binary_logloss: 0.664631\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\tTrain's binary_logloss: 0.62924\tValidate's binary_logloss: 0.661538\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\tTrain's binary_logloss: 0.628514\tValidate's binary_logloss: 0.661691\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\tTrain's binary_logloss: 0.627801\tValidate's binary_logloss: 0.66185\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\tTrain's binary_logloss: 0.627102\tValidate's binary_logloss: 0.662015\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tTrain's binary_logloss: 0.626417\tValidate's binary_logloss: 0.662185\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\tTrain's binary_logloss: 0.625745\tValidate's binary_logloss: 0.66236\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\tTrain's binary_logloss: 0.625086\tValidate's binary_logloss: 0.66254\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\tTrain's binary_logloss: 0.621662\tValidate's binary_logloss: 0.659558\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\tTrain's binary_logloss: 0.618305\tValidate's binary_logloss: 0.656643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tTrain's binary_logloss: 0.617675\tValidate's binary_logloss: 0.65683\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\tTrain's binary_logloss: 0.617057\tValidate's binary_logloss: 0.657022\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tTrain's binary_logloss: 0.616451\tValidate's binary_logloss: 0.657219\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tTrain's binary_logloss: 0.615856\tValidate's binary_logloss: 0.657419\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\tTrain's binary_logloss: 0.615273\tValidate's binary_logloss: 0.657624\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tTrain's binary_logloss: 0.612015\tValidate's binary_logloss: 0.654803\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\tTrain's binary_logloss: 0.61145\tValidate's binary_logloss: 0.655012\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\tTrain's binary_logloss: 0.610896\tValidate's binary_logloss: 0.655225\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tTrain's binary_logloss: 0.610353\tValidate's binary_logloss: 0.655441\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\tTrain's binary_logloss: 0.607176\tValidate's binary_logloss: 0.652698\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\tTrain's binary_logloss: 0.60665\tValidate's binary_logloss: 0.652918\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\tTrain's binary_logloss: 0.606134\tValidate's binary_logloss: 0.653141\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\tTrain's binary_logloss: 0.605628\tValidate's binary_logloss: 0.653368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[58]\tTrain's binary_logloss: 0.605132\tValidate's binary_logloss: 0.653597\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\tTrain's binary_logloss: 0.602038\tValidate's binary_logloss: 0.650934\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\tTrain's binary_logloss: 0.601557\tValidate's binary_logloss: 0.651166\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\tTrain's binary_logloss: 0.601086\tValidate's binary_logloss: 0.651401\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\tTrain's binary_logloss: 0.600623\tValidate's binary_logloss: 0.651638\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[63]\tTrain's binary_logloss: 0.60017\tValidate's binary_logloss: 0.651877\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\tTrain's binary_logloss: 0.597155\tValidate's binary_logloss: 0.649291\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\tTrain's binary_logloss: 0.596716\tValidate's binary_logloss: 0.649532\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\tTrain's binary_logloss: 0.596285\tValidate's binary_logloss: 0.649776\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[67]\tTrain's binary_logloss: 0.595862\tValidate's binary_logloss: 0.650021\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[68]\tTrain's binary_logloss: 0.595448\tValidate's binary_logloss: 0.650269\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\tTrain's binary_logloss: 0.595041\tValidate's binary_logloss: 0.650518\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\tTrain's binary_logloss: 0.594642\tValidate's binary_logloss: 0.650769\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\tTrain's binary_logloss: 0.59425\tValidate's binary_logloss: 0.651022\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\tTrain's binary_logloss: 0.593866\tValidate's binary_logloss: 0.651276\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\tTrain's binary_logloss: 0.593489\tValidate's binary_logloss: 0.651531\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\tTrain's binary_logloss: 0.59312\tValidate's binary_logloss: 0.651788\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[75]\tTrain's binary_logloss: 0.592757\tValidate's binary_logloss: 0.652047\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\tTrain's binary_logloss: 0.589846\tValidate's binary_logloss: 0.649557\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\tTrain's binary_logloss: 0.589458\tValidate's binary_logloss: 0.649603\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[78]\tTrain's binary_logloss: 0.589077\tValidate's binary_logloss: 0.649652\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\tTrain's binary_logloss: 0.586228\tValidate's binary_logloss: 0.647221\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\tTrain's binary_logloss: 0.585858\tValidate's binary_logloss: 0.647273\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\tTrain's binary_logloss: 0.583066\tValidate's binary_logloss: 0.644898\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\tTrain's binary_logloss: 0.582738\tValidate's binary_logloss: 0.645161\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\tTrain's binary_logloss: 0.580003\tValidate's binary_logloss: 0.642839\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[84]\tTrain's binary_logloss: 0.579651\tValidate's binary_logloss: 0.642896\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[85]\tTrain's binary_logloss: 0.579306\tValidate's binary_logloss: 0.642956\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\tTrain's binary_logloss: 0.576628\tValidate's binary_logloss: 0.640688\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[87]\tTrain's binary_logloss: 0.574\tValidate's binary_logloss: 0.63847\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[88]\tTrain's binary_logloss: 0.571421\tValidate's binary_logloss: 0.636299\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[89]\tTrain's binary_logloss: 0.571092\tValidate's binary_logloss: 0.636362\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[90]\tTrain's binary_logloss: 0.570769\tValidate's binary_logloss: 0.636427\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[91]\tTrain's binary_logloss: 0.570482\tValidate's binary_logloss: 0.636693\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[92]\tTrain's binary_logloss: 0.567959\tValidate's binary_logloss: 0.634575\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[93]\tTrain's binary_logloss: 0.567649\tValidate's binary_logloss: 0.634644\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[94]\tTrain's binary_logloss: 0.565176\tValidate's binary_logloss: 0.632573\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[95]\tTrain's binary_logloss: 0.564897\tValidate's binary_logloss: 0.63242\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[96]\tTrain's binary_logloss: 0.564599\tValidate's binary_logloss: 0.632494\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[97]\tTrain's binary_logloss: 0.564328\tValidate's binary_logloss: 0.632346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[98]\tTrain's binary_logloss: 0.564039\tValidate's binary_logloss: 0.632424\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\tTrain's binary_logloss: 0.563777\tValidate's binary_logloss: 0.632282\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tTrain's binary_logloss: 0.563497\tValidate's binary_logloss: 0.632364\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[101]\tTrain's binary_logloss: 0.563247\tValidate's binary_logloss: 0.632639\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[102]\tTrain's binary_logloss: 0.562975\tValidate's binary_logloss: 0.632724\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[103]\tTrain's binary_logloss: 0.562727\tValidate's binary_logloss: 0.632588\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[104]\tTrain's binary_logloss: 0.562463\tValidate's binary_logloss: 0.632677\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[105]\tTrain's binary_logloss: 0.562227\tValidate's binary_logloss: 0.632956\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[106]\tTrain's binary_logloss: 0.56199\tValidate's binary_logloss: 0.632825\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[107]\tTrain's binary_logloss: 0.561762\tValidate's binary_logloss: 0.632982\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[108]\tTrain's binary_logloss: 0.56073\tValidate's binary_logloss: 0.632734\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[109]\tTrain's binary_logloss: 0.560507\tValidate's binary_logloss: 0.633015\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[110]\tTrain's binary_logloss: 0.559498\tValidate's binary_logloss: 0.632782\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[111]\tTrain's binary_logloss: 0.558507\tValidate's binary_logloss: 0.632562\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[112]\tTrain's binary_logloss: 0.557536\tValidate's binary_logloss: 0.632354\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[113]\tTrain's binary_logloss: 0.555161\tValidate's binary_logloss: 0.630376\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[114]\tTrain's binary_logloss: 0.554214\tValidate's binary_logloss: 0.630183\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[115]\tTrain's binary_logloss: 0.553285\tValidate's binary_logloss: 0.630002\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[116]\tTrain's binary_logloss: 0.550964\tValidate's binary_logloss: 0.628076\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[117]\tTrain's binary_logloss: 0.548687\tValidate's binary_logloss: 0.626192\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[118]\tTrain's binary_logloss: 0.547785\tValidate's binary_logloss: 0.626028\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[119]\tTrain's binary_logloss: 0.547086\tValidate's binary_logloss: 0.626004\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[120]\tTrain's binary_logloss: 0.546399\tValidate's binary_logloss: 0.625988\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[121]\tTrain's binary_logloss: 0.545726\tValidate's binary_logloss: 0.625978\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[122]\tTrain's binary_logloss: 0.545066\tValidate's binary_logloss: 0.625974\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[123]\tTrain's binary_logloss: 0.542851\tValidate's binary_logloss: 0.624144\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[124]\tTrain's binary_logloss: 0.542158\tValidate's binary_logloss: 0.624355\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[125]\tTrain's binary_logloss: 0.539987\tValidate's binary_logloss: 0.622567\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[126]\tTrain's binary_logloss: 0.53931\tValidate's binary_logloss: 0.622784\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[127]\tTrain's binary_logloss: 0.537182\tValidate's binary_logloss: 0.621036\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[128]\tTrain's binary_logloss: 0.536522\tValidate's binary_logloss: 0.621258\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[129]\tTrain's binary_logloss: 0.535874\tValidate's binary_logloss: 0.621484\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[130]\tTrain's binary_logloss: 0.535239\tValidate's binary_logloss: 0.621713\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[131]\tTrain's binary_logloss: 0.533159\tValidate's binary_logloss: 0.620011\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[132]\tTrain's binary_logloss: 0.532539\tValidate's binary_logloss: 0.620245\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[133]\tTrain's binary_logloss: 0.531975\tValidate's binary_logloss: 0.620279\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[134]\tTrain's binary_logloss: 0.529938\tValidate's binary_logloss: 0.618617\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[135]\tTrain's binary_logloss: 0.527938\tValidate's binary_logloss: 0.616991\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[136]\tTrain's binary_logloss: 0.527346\tValidate's binary_logloss: 0.617232\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[137]\tTrain's binary_logloss: 0.526766\tValidate's binary_logloss: 0.617476\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[138]\tTrain's binary_logloss: 0.526196\tValidate's binary_logloss: 0.617724\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[139]\tTrain's binary_logloss: 0.52424\tValidate's binary_logloss: 0.616138\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[140]\tTrain's binary_logloss: 0.522318\tValidate's binary_logloss: 0.614586\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[141]\tTrain's binary_logloss: 0.521765\tValidate's binary_logloss: 0.614838\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[142]\tTrain's binary_logloss: 0.521223\tValidate's binary_logloss: 0.615092\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[143]\tTrain's binary_logloss: 0.52069\tValidate's binary_logloss: 0.615348\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[144]\tTrain's binary_logloss: 0.520168\tValidate's binary_logloss: 0.615607\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[145]\tTrain's binary_logloss: 0.519655\tValidate's binary_logloss: 0.615869\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[146]\tTrain's binary_logloss: 0.519153\tValidate's binary_logloss: 0.616132\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[147]\tTrain's binary_logloss: 0.517279\tValidate's binary_logloss: 0.614625\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[148]\tTrain's binary_logloss: 0.515438\tValidate's binary_logloss: 0.613149\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[149]\tTrain's binary_logloss: 0.513629\tValidate's binary_logloss: 0.611705\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[150]\tTrain's binary_logloss: 0.513143\tValidate's binary_logloss: 0.611972\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[151]\tTrain's binary_logloss: 0.512666\tValidate's binary_logloss: 0.61224\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[152]\tTrain's binary_logloss: 0.512198\tValidate's binary_logloss: 0.612511\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[153]\tTrain's binary_logloss: 0.511739\tValidate's binary_logloss: 0.612783\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[154]\tTrain's binary_logloss: 0.50997\tValidate's binary_logloss: 0.611376\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[155]\tTrain's binary_logloss: 0.508232\tValidate's binary_logloss: 0.609998\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[156]\tTrain's binary_logloss: 0.507785\tValidate's binary_logloss: 0.610273\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[157]\tTrain's binary_logloss: 0.507348\tValidate's binary_logloss: 0.610548\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[158]\tTrain's binary_logloss: 0.506918\tValidate's binary_logloss: 0.610826\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[159]\tTrain's binary_logloss: 0.505215\tValidate's binary_logloss: 0.609482\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[160]\tTrain's binary_logloss: 0.504795\tValidate's binary_logloss: 0.60976\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[161]\tTrain's binary_logloss: 0.504383\tValidate's binary_logloss: 0.61004\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[162]\tTrain's binary_logloss: 0.503979\tValidate's binary_logloss: 0.610321\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[163]\tTrain's binary_logloss: 0.50231\tValidate's binary_logloss: 0.609009\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[164]\tTrain's binary_logloss: 0.501916\tValidate's binary_logloss: 0.609291\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[165]\tTrain's binary_logloss: 0.501528\tValidate's binary_logloss: 0.609574\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[166]\tTrain's binary_logloss: 0.499891\tValidate's binary_logloss: 0.608292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[167]\tTrain's binary_logloss: 0.499513\tValidate's binary_logloss: 0.608576\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[168]\tTrain's binary_logloss: 0.499141\tValidate's binary_logloss: 0.60886\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[169]\tTrain's binary_logloss: 0.498777\tValidate's binary_logloss: 0.609146\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[170]\tTrain's binary_logloss: 0.49842\tValidate's binary_logloss: 0.609431\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[171]\tTrain's binary_logloss: 0.498069\tValidate's binary_logloss: 0.609718\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[172]\tTrain's binary_logloss: 0.497725\tValidate's binary_logloss: 0.610005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[173]\tTrain's binary_logloss: 0.497387\tValidate's binary_logloss: 0.610292\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[174]\tTrain's binary_logloss: 0.497055\tValidate's binary_logloss: 0.610579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[175]\tTrain's binary_logloss: 0.496755\tValidate's binary_logloss: 0.610694\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[176]\tTrain's binary_logloss: 0.496435\tValidate's binary_logloss: 0.610982\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[177]\tTrain's binary_logloss: 0.496121\tValidate's binary_logloss: 0.611269\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[178]\tTrain's binary_logloss: 0.495813\tValidate's binary_logloss: 0.611557\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[179]\tTrain's binary_logloss: 0.495534\tValidate's binary_logloss: 0.611678\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[180]\tTrain's binary_logloss: 0.495237\tValidate's binary_logloss: 0.611965\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[181]\tTrain's binary_logloss: 0.494945\tValidate's binary_logloss: 0.612253\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[182]\tTrain's binary_logloss: 0.494659\tValidate's binary_logloss: 0.612541\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[183]\tTrain's binary_logloss: 0.4944\tValidate's binary_logloss: 0.612665\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[184]\tTrain's binary_logloss: 0.494124\tValidate's binary_logloss: 0.612952\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[185]\tTrain's binary_logloss: 0.493853\tValidate's binary_logloss: 0.613239\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[186]\tTrain's binary_logloss: 0.493587\tValidate's binary_logloss: 0.613526\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[187]\tTrain's binary_logloss: 0.493346\tValidate's binary_logloss: 0.613654\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[188]\tTrain's binary_logloss: 0.49309\tValidate's binary_logloss: 0.61394\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[189]\tTrain's binary_logloss: 0.491501\tValidate's binary_logloss: 0.6127\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[190]\tTrain's binary_logloss: 0.489939\tValidate's binary_logloss: 0.611487\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[191]\tTrain's binary_logloss: 0.489689\tValidate's binary_logloss: 0.611771\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[192]\tTrain's binary_logloss: 0.488154\tValidate's binary_logloss: 0.610584\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[193]\tTrain's binary_logloss: 0.486646\tValidate's binary_logloss: 0.609423\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[194]\tTrain's binary_logloss: 0.485163\tValidate's binary_logloss: 0.608286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[195]\tTrain's binary_logloss: 0.483705\tValidate's binary_logloss: 0.607173\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[196]\tTrain's binary_logloss: 0.482272\tValidate's binary_logloss: 0.606085\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[197]\tTrain's binary_logloss: 0.482031\tValidate's binary_logloss: 0.606367\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[198]\tTrain's binary_logloss: 0.481795\tValidate's binary_logloss: 0.60665\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[199]\tTrain's binary_logloss: 0.480387\tValidate's binary_logloss: 0.605585\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tTrain's binary_logloss: 0.479002\tValidate's binary_logloss: 0.604544\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[201]\tTrain's binary_logloss: 0.478771\tValidate's binary_logloss: 0.604825\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[202]\tTrain's binary_logloss: 0.478545\tValidate's binary_logloss: 0.605106\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[203]\tTrain's binary_logloss: 0.478323\tValidate's binary_logloss: 0.605386\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[204]\tTrain's binary_logloss: 0.478104\tValidate's binary_logloss: 0.605665\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[205]\tTrain's binary_logloss: 0.476744\tValidate's binary_logloss: 0.604647\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[206]\tTrain's binary_logloss: 0.476627\tValidate's binary_logloss: 0.604697\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[207]\tTrain's binary_logloss: 0.475297\tValidate's binary_logloss: 0.603725\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[208]\tTrain's binary_logloss: 0.47399\tValidate's binary_logloss: 0.602775\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[209]\tTrain's binary_logloss: 0.472704\tValidate's binary_logloss: 0.601846\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[210]\tTrain's binary_logloss: 0.471439\tValidate's binary_logloss: 0.600937\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[211]\tTrain's binary_logloss: 0.471229\tValidate's binary_logloss: 0.601215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[212]\tTrain's binary_logloss: 0.469986\tValidate's binary_logloss: 0.600326\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[213]\tTrain's binary_logloss: 0.469871\tValidate's binary_logloss: 0.600379\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[214]\tTrain's binary_logloss: 0.469666\tValidate's binary_logloss: 0.600656\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[215]\tTrain's binary_logloss: 0.469554\tValidate's binary_logloss: 0.600709\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[216]\tTrain's binary_logloss: 0.469353\tValidate's binary_logloss: 0.600985\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[217]\tTrain's binary_logloss: 0.468131\tValidate's binary_logloss: 0.600118\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[218]\tTrain's binary_logloss: 0.468022\tValidate's binary_logloss: 0.600173\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[219]\tTrain's binary_logloss: 0.467826\tValidate's binary_logloss: 0.600448\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[220]\tTrain's binary_logloss: 0.467719\tValidate's binary_logloss: 0.600503\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[221]\tTrain's binary_logloss: 0.467527\tValidate's binary_logloss: 0.600777\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[222]\tTrain's binary_logloss: 0.467423\tValidate's binary_logloss: 0.600833\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[223]\tTrain's binary_logloss: 0.467235\tValidate's binary_logloss: 0.601106\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[224]\tTrain's binary_logloss: 0.467133\tValidate's binary_logloss: 0.601163\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[225]\tTrain's binary_logloss: 0.465932\tValidate's binary_logloss: 0.600317\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[226]\tTrain's binary_logloss: 0.464751\tValidate's binary_logloss: 0.599491\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[227]\tTrain's binary_logloss: 0.464567\tValidate's binary_logloss: 0.599763\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[228]\tTrain's binary_logloss: 0.463406\tValidate's binary_logloss: 0.598955\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[229]\tTrain's binary_logloss: 0.463306\tValidate's binary_logloss: 0.599014\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[230]\tTrain's binary_logloss: 0.463127\tValidate's binary_logloss: 0.599284\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[231]\tTrain's binary_logloss: 0.461984\tValidate's binary_logloss: 0.598495\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[232]\tTrain's binary_logloss: 0.461887\tValidate's binary_logloss: 0.598554\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[233]\tTrain's binary_logloss: 0.461712\tValidate's binary_logloss: 0.598824\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[234]\tTrain's binary_logloss: 0.460588\tValidate's binary_logloss: 0.598053\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[235]\tTrain's binary_logloss: 0.460493\tValidate's binary_logloss: 0.598113\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[236]\tTrain's binary_logloss: 0.460322\tValidate's binary_logloss: 0.598381\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[237]\tTrain's binary_logloss: 0.460229\tValidate's binary_logloss: 0.598441\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[238]\tTrain's binary_logloss: 0.460061\tValidate's binary_logloss: 0.598708\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[239]\tTrain's binary_logloss: 0.45997\tValidate's binary_logloss: 0.598768\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[240]\tTrain's binary_logloss: 0.459806\tValidate's binary_logloss: 0.599034\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[241]\tTrain's binary_logloss: 0.458701\tValidate's binary_logloss: 0.598283\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[242]\tTrain's binary_logloss: 0.458612\tValidate's binary_logloss: 0.598344\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[243]\tTrain's binary_logloss: 0.458452\tValidate's binary_logloss: 0.598608\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[244]\tTrain's binary_logloss: 0.458365\tValidate's binary_logloss: 0.598669\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[245]\tTrain's binary_logloss: 0.457277\tValidate's binary_logloss: 0.597936\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[246]\tTrain's binary_logloss: 0.456208\tValidate's binary_logloss: 0.59722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[247]\tTrain's binary_logloss: 0.456051\tValidate's binary_logloss: 0.597483\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[248]\tTrain's binary_logloss: 0.454999\tValidate's binary_logloss: 0.596783\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[249]\tTrain's binary_logloss: 0.454914\tValidate's binary_logloss: 0.596845\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[250]\tTrain's binary_logloss: 0.453879\tValidate's binary_logloss: 0.596162\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[251]\tTrain's binary_logloss: 0.45286\tValidate's binary_logloss: 0.595495\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[252]\tTrain's binary_logloss: 0.451859\tValidate's binary_logloss: 0.594843\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[253]\tTrain's binary_logloss: 0.451706\tValidate's binary_logloss: 0.595104\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[254]\tTrain's binary_logloss: 0.450721\tValidate's binary_logloss: 0.594468\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[255]\tTrain's binary_logloss: 0.449751\tValidate's binary_logloss: 0.593847\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[256]\tTrain's binary_logloss: 0.449602\tValidate's binary_logloss: 0.594105\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[257]\tTrain's binary_logloss: 0.449518\tValidate's binary_logloss: 0.59417\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[258]\tTrain's binary_logloss: 0.449372\tValidate's binary_logloss: 0.594427\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[259]\tTrain's binary_logloss: 0.448418\tValidate's binary_logloss: 0.593821\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[260]\tTrain's binary_logloss: 0.447479\tValidate's binary_logloss: 0.59323\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[261]\tTrain's binary_logloss: 0.447396\tValidate's binary_logloss: 0.593295\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[262]\tTrain's binary_logloss: 0.446472\tValidate's binary_logloss: 0.592719\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[263]\tTrain's binary_logloss: 0.44633\tValidate's binary_logloss: 0.592974\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[264]\tTrain's binary_logloss: 0.445421\tValidate's binary_logloss: 0.592411\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[265]\tTrain's binary_logloss: 0.445339\tValidate's binary_logloss: 0.592477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[266]\tTrain's binary_logloss: 0.445199\tValidate's binary_logloss: 0.59273\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[267]\tTrain's binary_logloss: 0.44512\tValidate's binary_logloss: 0.592796\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[268]\tTrain's binary_logloss: 0.444983\tValidate's binary_logloss: 0.593048\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[269]\tTrain's binary_logloss: 0.444905\tValidate's binary_logloss: 0.593114\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[270]\tTrain's binary_logloss: 0.444011\tValidate's binary_logloss: 0.592566\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[271]\tTrain's binary_logloss: 0.443876\tValidate's binary_logloss: 0.592817\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[272]\tTrain's binary_logloss: 0.4438\tValidate's binary_logloss: 0.592882\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[273]\tTrain's binary_logloss: 0.44292\tValidate's binary_logloss: 0.592348\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[274]\tTrain's binary_logloss: 0.442788\tValidate's binary_logloss: 0.592598\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[275]\tTrain's binary_logloss: 0.442713\tValidate's binary_logloss: 0.592663\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[276]\tTrain's binary_logloss: 0.442584\tValidate's binary_logloss: 0.592911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[277]\tTrain's binary_logloss: 0.441718\tValidate's binary_logloss: 0.592391\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[278]\tTrain's binary_logloss: 0.441644\tValidate's binary_logloss: 0.592457\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[279]\tTrain's binary_logloss: 0.441518\tValidate's binary_logloss: 0.592703\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[280]\tTrain's binary_logloss: 0.441446\tValidate's binary_logloss: 0.592769\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[281]\tTrain's binary_logloss: 0.441323\tValidate's binary_logloss: 0.593014\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[282]\tTrain's binary_logloss: 0.440469\tValidate's binary_logloss: 0.592507\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[283]\tTrain's binary_logloss: 0.440399\tValidate's binary_logloss: 0.592572\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[284]\tTrain's binary_logloss: 0.440278\tValidate's binary_logloss: 0.592815\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[285]\tTrain's binary_logloss: 0.440209\tValidate's binary_logloss: 0.59288\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[286]\tTrain's binary_logloss: 0.44009\tValidate's binary_logloss: 0.593122\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[287]\tTrain's binary_logloss: 0.439241\tValidate's binary_logloss: 0.592606\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[288]\tTrain's binary_logloss: 0.438406\tValidate's binary_logloss: 0.592103\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[289]\tTrain's binary_logloss: 0.438338\tValidate's binary_logloss: 0.592168\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[290]\tTrain's binary_logloss: 0.438222\tValidate's binary_logloss: 0.592408\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[291]\tTrain's binary_logloss: 0.438155\tValidate's binary_logloss: 0.592474\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[292]\tTrain's binary_logloss: 0.438056\tValidate's binary_logloss: 0.592589\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[293]\tTrain's binary_logloss: 0.437991\tValidate's binary_logloss: 0.592654\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[294]\tTrain's binary_logloss: 0.437894\tValidate's binary_logloss: 0.59277\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[295]\tTrain's binary_logloss: 0.437798\tValidate's binary_logloss: 0.592885\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[296]\tTrain's binary_logloss: 0.437705\tValidate's binary_logloss: 0.593\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[297]\tTrain's binary_logloss: 0.437613\tValidate's binary_logloss: 0.593115\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[298]\tTrain's binary_logloss: 0.437522\tValidate's binary_logloss: 0.59323\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[299]\tTrain's binary_logloss: 0.437459\tValidate's binary_logloss: 0.593295\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[300]\tTrain's binary_logloss: 0.436635\tValidate's binary_logloss: 0.592803\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[301]\tTrain's binary_logloss: 0.435825\tValidate's binary_logloss: 0.592324\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[302]\tTrain's binary_logloss: 0.435028\tValidate's binary_logloss: 0.591857\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[303]\tTrain's binary_logloss: 0.43494\tValidate's binary_logloss: 0.591969\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[304]\tTrain's binary_logloss: 0.434853\tValidate's binary_logloss: 0.592082\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[305]\tTrain's binary_logloss: 0.434068\tValidate's binary_logloss: 0.591626\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[306]\tTrain's binary_logloss: 0.433295\tValidate's binary_logloss: 0.591183\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[307]\tTrain's binary_logloss: 0.43321\tValidate's binary_logloss: 0.591293\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[308]\tTrain's binary_logloss: 0.433127\tValidate's binary_logloss: 0.591403\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[309]\tTrain's binary_logloss: 0.433045\tValidate's binary_logloss: 0.591513\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[310]\tTrain's binary_logloss: 0.432965\tValidate's binary_logloss: 0.591623\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[311]\tTrain's binary_logloss: 0.432886\tValidate's binary_logloss: 0.591733\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[312]\tTrain's binary_logloss: 0.432854\tValidate's binary_logloss: 0.591763\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[313]\tTrain's binary_logloss: 0.432093\tValidate's binary_logloss: 0.59133\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[314]\tTrain's binary_logloss: 0.431345\tValidate's binary_logloss: 0.590908\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[315]\tTrain's binary_logloss: 0.431267\tValidate's binary_logloss: 0.591015\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[316]\tTrain's binary_logloss: 0.431191\tValidate's binary_logloss: 0.591122\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[317]\tTrain's binary_logloss: 0.431159\tValidate's binary_logloss: 0.591149\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[318]\tTrain's binary_logloss: 0.431084\tValidate's binary_logloss: 0.591255\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[319]\tTrain's binary_logloss: 0.430347\tValidate's binary_logloss: 0.590843\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[320]\tTrain's binary_logloss: 0.430316\tValidate's binary_logloss: 0.590869\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[321]\tTrain's binary_logloss: 0.430242\tValidate's binary_logloss: 0.590973\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[322]\tTrain's binary_logloss: 0.430211\tValidate's binary_logloss: 0.590997\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[323]\tTrain's binary_logloss: 0.430146\tValidate's binary_logloss: 0.591063\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[324]\tTrain's binary_logloss: 0.429421\tValidate's binary_logloss: 0.590663\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[325]\tTrain's binary_logloss: 0.428708\tValidate's binary_logloss: 0.590273\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[326]\tTrain's binary_logloss: 0.428635\tValidate's binary_logloss: 0.590375\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[327]\tTrain's binary_logloss: 0.428563\tValidate's binary_logloss: 0.590477\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[328]\tTrain's binary_logloss: 0.428532\tValidate's binary_logloss: 0.590498\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[329]\tTrain's binary_logloss: 0.427864\tValidate's binary_logloss: 0.590063\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[330]\tTrain's binary_logloss: 0.427793\tValidate's binary_logloss: 0.590163\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[331]\tTrain's binary_logloss: 0.427135\tValidate's binary_logloss: 0.589738\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[332]\tTrain's binary_logloss: 0.427066\tValidate's binary_logloss: 0.589837\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[333]\tTrain's binary_logloss: 0.427036\tValidate's binary_logloss: 0.589855\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[334]\tTrain's binary_logloss: 0.426967\tValidate's binary_logloss: 0.589952\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[335]\tTrain's binary_logloss: 0.426938\tValidate's binary_logloss: 0.589969\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[336]\tTrain's binary_logloss: 0.42687\tValidate's binary_logloss: 0.590066\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[337]\tTrain's binary_logloss: 0.426841\tValidate's binary_logloss: 0.590081\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[338]\tTrain's binary_logloss: 0.426774\tValidate's binary_logloss: 0.590177\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[339]\tTrain's binary_logloss: 0.426744\tValidate's binary_logloss: 0.590192\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[340]\tTrain's binary_logloss: 0.426098\tValidate's binary_logloss: 0.589777\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[341]\tTrain's binary_logloss: 0.425461\tValidate's binary_logloss: 0.589371\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[342]\tTrain's binary_logloss: 0.425395\tValidate's binary_logloss: 0.589464\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[343]\tTrain's binary_logloss: 0.42533\tValidate's binary_logloss: 0.589557\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[344]\tTrain's binary_logloss: 0.424703\tValidate's binary_logloss: 0.589161\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[345]\tTrain's binary_logloss: 0.424639\tValidate's binary_logloss: 0.589253\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[346]\tTrain's binary_logloss: 0.424023\tValidate's binary_logloss: 0.588866\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[347]\tTrain's binary_logloss: 0.423994\tValidate's binary_logloss: 0.588875\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[348]\tTrain's binary_logloss: 0.423931\tValidate's binary_logloss: 0.588965\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[349]\tTrain's binary_logloss: 0.423903\tValidate's binary_logloss: 0.588973\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[350]\tTrain's binary_logloss: 0.42384\tValidate's binary_logloss: 0.589062\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[351]\tTrain's binary_logloss: 0.423234\tValidate's binary_logloss: 0.588685\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[352]\tTrain's binary_logloss: 0.422637\tValidate's binary_logloss: 0.588317\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[353]\tTrain's binary_logloss: 0.422575\tValidate's binary_logloss: 0.588404\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[354]\tTrain's binary_logloss: 0.422515\tValidate's binary_logloss: 0.58849\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[355]\tTrain's binary_logloss: 0.421927\tValidate's binary_logloss: 0.588131\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[356]\tTrain's binary_logloss: 0.421899\tValidate's binary_logloss: 0.588135\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[357]\tTrain's binary_logloss: 0.421839\tValidate's binary_logloss: 0.588219\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[358]\tTrain's binary_logloss: 0.421811\tValidate's binary_logloss: 0.588222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[359]\tTrain's binary_logloss: 0.421233\tValidate's binary_logloss: 0.587872\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[360]\tTrain's binary_logloss: 0.421174\tValidate's binary_logloss: 0.587955\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[361]\tTrain's binary_logloss: 0.421115\tValidate's binary_logloss: 0.588038\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[362]\tTrain's binary_logloss: 0.421026\tValidate's binary_logloss: 0.588101\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[363]\tTrain's binary_logloss: 0.420422\tValidate's binary_logloss: 0.587817\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[364]\tTrain's binary_logloss: 0.420365\tValidate's binary_logloss: 0.587899\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[365]\tTrain's binary_logloss: 0.41977\tValidate's binary_logloss: 0.587624\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[366]\tTrain's binary_logloss: 0.419185\tValidate's binary_logloss: 0.587358\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[367]\tTrain's binary_logloss: 0.419097\tValidate's binary_logloss: 0.587421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[368]\tTrain's binary_logloss: 0.419041\tValidate's binary_logloss: 0.587502\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[369]\tTrain's binary_logloss: 0.419013\tValidate's binary_logloss: 0.5875\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[370]\tTrain's binary_logloss: 0.418957\tValidate's binary_logloss: 0.58758\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[371]\tTrain's binary_logloss: 0.418929\tValidate's binary_logloss: 0.587577\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[372]\tTrain's binary_logloss: 0.418843\tValidate's binary_logloss: 0.58764\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[373]\tTrain's binary_logloss: 0.418787\tValidate's binary_logloss: 0.58772\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[374]\tTrain's binary_logloss: 0.418212\tValidate's binary_logloss: 0.587463\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[375]\tTrain's binary_logloss: 0.417645\tValidate's binary_logloss: 0.587214\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[376]\tTrain's binary_logloss: 0.417561\tValidate's binary_logloss: 0.587277\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[377]\tTrain's binary_logloss: 0.417004\tValidate's binary_logloss: 0.587036\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[378]\tTrain's binary_logloss: 0.416948\tValidate's binary_logloss: 0.587114\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[379]\tTrain's binary_logloss: 0.416894\tValidate's binary_logloss: 0.587192\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[380]\tTrain's binary_logloss: 0.416867\tValidate's binary_logloss: 0.587186\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[381]\tTrain's binary_logloss: 0.416317\tValidate's binary_logloss: 0.586954\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[382]\tTrain's binary_logloss: 0.416235\tValidate's binary_logloss: 0.587016\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[383]\tTrain's binary_logloss: 0.415695\tValidate's binary_logloss: 0.586792\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[384]\tTrain's binary_logloss: 0.415163\tValidate's binary_logloss: 0.586575\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[385]\tTrain's binary_logloss: 0.415108\tValidate's binary_logloss: 0.586651\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[386]\tTrain's binary_logloss: 0.414584\tValidate's binary_logloss: 0.586442\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[387]\tTrain's binary_logloss: 0.414068\tValidate's binary_logloss: 0.58624\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[388]\tTrain's binary_logloss: 0.41356\tValidate's binary_logloss: 0.586046\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[389]\tTrain's binary_logloss: 0.41348\tValidate's binary_logloss: 0.586108\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[390]\tTrain's binary_logloss: 0.413426\tValidate's binary_logloss: 0.586182\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[391]\tTrain's binary_logloss: 0.413347\tValidate's binary_logloss: 0.586244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[392]\tTrain's binary_logloss: 0.412847\tValidate's binary_logloss: 0.586058\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[393]\tTrain's binary_logloss: 0.412794\tValidate's binary_logloss: 0.586131\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[394]\tTrain's binary_logloss: 0.412301\tValidate's binary_logloss: 0.585952\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[395]\tTrain's binary_logloss: 0.412224\tValidate's binary_logloss: 0.586014\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[396]\tTrain's binary_logloss: 0.411739\tValidate's binary_logloss: 0.585841\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[397]\tTrain's binary_logloss: 0.411262\tValidate's binary_logloss: 0.585676\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[398]\tTrain's binary_logloss: 0.411209\tValidate's binary_logloss: 0.585748\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[399]\tTrain's binary_logloss: 0.410739\tValidate's binary_logloss: 0.585589\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[400]\tTrain's binary_logloss: 0.410663\tValidate's binary_logloss: 0.585651\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[401]\tTrain's binary_logloss: 0.410611\tValidate's binary_logloss: 0.585722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[402]\tTrain's binary_logloss: 0.410559\tValidate's binary_logloss: 0.585793\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403]\tTrain's binary_logloss: 0.410485\tValidate's binary_logloss: 0.585855\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[404]\tTrain's binary_logloss: 0.410022\tValidate's binary_logloss: 0.585703\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[405]\tTrain's binary_logloss: 0.409971\tValidate's binary_logloss: 0.585773\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[406]\tTrain's binary_logloss: 0.409897\tValidate's binary_logloss: 0.585835\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[407]\tTrain's binary_logloss: 0.409441\tValidate's binary_logloss: 0.58569\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[408]\tTrain's binary_logloss: 0.409391\tValidate's binary_logloss: 0.58576\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[409]\tTrain's binary_logloss: 0.409318\tValidate's binary_logloss: 0.585749\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[410]\tTrain's binary_logloss: 0.409291\tValidate's binary_logloss: 0.585734\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[411]\tTrain's binary_logloss: 0.409241\tValidate's binary_logloss: 0.585804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[412]\tTrain's binary_logloss: 0.408792\tValidate's binary_logloss: 0.585665\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[413]\tTrain's binary_logloss: 0.408349\tValidate's binary_logloss: 0.585532\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[414]\tTrain's binary_logloss: 0.408277\tValidate's binary_logloss: 0.585594\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[415]\tTrain's binary_logloss: 0.408228\tValidate's binary_logloss: 0.585663\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[416]\tTrain's binary_logloss: 0.407792\tValidate's binary_logloss: 0.585536\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[417]\tTrain's binary_logloss: 0.40772\tValidate's binary_logloss: 0.585525\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[418]\tTrain's binary_logloss: 0.407291\tValidate's binary_logloss: 0.585404\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[419]\tTrain's binary_logloss: 0.407241\tValidate's binary_logloss: 0.585472\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[420]\tTrain's binary_logloss: 0.407171\tValidate's binary_logloss: 0.585462\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[421]\tTrain's binary_logloss: 0.407102\tValidate's binary_logloss: 0.585453\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[422]\tTrain's binary_logloss: 0.407034\tValidate's binary_logloss: 0.585444\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[423]\tTrain's binary_logloss: 0.406968\tValidate's binary_logloss: 0.585436\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[424]\tTrain's binary_logloss: 0.406942\tValidate's binary_logloss: 0.585421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[425]\tTrain's binary_logloss: 0.406518\tValidate's binary_logloss: 0.585307\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[426]\tTrain's binary_logloss: 0.406101\tValidate's binary_logloss: 0.585198\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[427]\tTrain's binary_logloss: 0.406051\tValidate's binary_logloss: 0.585265\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[428]\tTrain's binary_logloss: 0.406003\tValidate's binary_logloss: 0.585332\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[429]\tTrain's binary_logloss: 0.405591\tValidate's binary_logloss: 0.585229\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[430]\tTrain's binary_logloss: 0.405526\tValidate's binary_logloss: 0.585221\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[431]\tTrain's binary_logloss: 0.405478\tValidate's binary_logloss: 0.585288\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[432]\tTrain's binary_logloss: 0.405451\tValidate's binary_logloss: 0.585271\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[433]\tTrain's binary_logloss: 0.405046\tValidate's binary_logloss: 0.585173\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[434]\tTrain's binary_logloss: 0.404978\tValidate's binary_logloss: 0.585238\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[435]\tTrain's binary_logloss: 0.40493\tValidate's binary_logloss: 0.585304\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[436]\tTrain's binary_logloss: 0.404865\tValidate's binary_logloss: 0.585297\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[437]\tTrain's binary_logloss: 0.404466\tValidate's binary_logloss: 0.585205\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[438]\tTrain's binary_logloss: 0.404073\tValidate's binary_logloss: 0.585118\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[439]\tTrain's binary_logloss: 0.404025\tValidate's binary_logloss: 0.585183\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[440]\tTrain's binary_logloss: 0.403962\tValidate's binary_logloss: 0.585177\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[441]\tTrain's binary_logloss: 0.403574\tValidate's binary_logloss: 0.585095\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[442]\tTrain's binary_logloss: 0.403512\tValidate's binary_logloss: 0.585089\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[443]\tTrain's binary_logloss: 0.403464\tValidate's binary_logloss: 0.585154\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[444]\tTrain's binary_logloss: 0.403403\tValidate's binary_logloss: 0.585148\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[445]\tTrain's binary_logloss: 0.403021\tValidate's binary_logloss: 0.585072\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[446]\tTrain's binary_logloss: 0.40296\tValidate's binary_logloss: 0.585067\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[447]\tTrain's binary_logloss: 0.402902\tValidate's binary_logloss: 0.585062\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[448]\tTrain's binary_logloss: 0.402524\tValidate's binary_logloss: 0.584991\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[449]\tTrain's binary_logloss: 0.402477\tValidate's binary_logloss: 0.585056\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[450]\tTrain's binary_logloss: 0.402105\tValidate's binary_logloss: 0.58499\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[451]\tTrain's binary_logloss: 0.401739\tValidate's binary_logloss: 0.584929\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[452]\tTrain's binary_logloss: 0.401692\tValidate's binary_logloss: 0.584993\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[453]\tTrain's binary_logloss: 0.401331\tValidate's binary_logloss: 0.584936\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[454]\tTrain's binary_logloss: 0.401267\tValidate's binary_logloss: 0.585005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[455]\tTrain's binary_logloss: 0.401209\tValidate's binary_logloss: 0.585001\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[456]\tTrain's binary_logloss: 0.401162\tValidate's binary_logloss: 0.585064\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[457]\tTrain's binary_logloss: 0.400806\tValidate's binary_logloss: 0.585013\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[458]\tTrain's binary_logloss: 0.400456\tValidate's binary_logloss: 0.584966\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[459]\tTrain's binary_logloss: 0.40011\tValidate's binary_logloss: 0.584925\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[460]\tTrain's binary_logloss: 0.400064\tValidate's binary_logloss: 0.584987\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[461]\tTrain's binary_logloss: 0.399724\tValidate's binary_logloss: 0.584949\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[462]\tTrain's binary_logloss: 0.399388\tValidate's binary_logloss: 0.584917\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[463]\tTrain's binary_logloss: 0.399058\tValidate's binary_logloss: 0.584889\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[464]\tTrain's binary_logloss: 0.398995\tValidate's binary_logloss: 0.584957\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[465]\tTrain's binary_logloss: 0.398949\tValidate's binary_logloss: 0.585018\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[466]\tTrain's binary_logloss: 0.398624\tValidate's binary_logloss: 0.584994\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[467]\tTrain's binary_logloss: 0.398303\tValidate's binary_logloss: 0.584975\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[468]\tTrain's binary_logloss: 0.398241\tValidate's binary_logloss: 0.585043\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[469]\tTrain's binary_logloss: 0.398195\tValidate's binary_logloss: 0.585102\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[470]\tTrain's binary_logloss: 0.398137\tValidate's binary_logloss: 0.585096\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[471]\tTrain's binary_logloss: 0.39808\tValidate's binary_logloss: 0.58509\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[472]\tTrain's binary_logloss: 0.398025\tValidate's binary_logloss: 0.585085\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[473]\tTrain's binary_logloss: 0.397979\tValidate's binary_logloss: 0.585145\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[474]\tTrain's binary_logloss: 0.397924\tValidate's binary_logloss: 0.58514\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[475]\tTrain's binary_logloss: 0.397871\tValidate's binary_logloss: 0.585136\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[476]\tTrain's binary_logloss: 0.397818\tValidate's binary_logloss: 0.585133\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[477]\tTrain's binary_logloss: 0.397767\tValidate's binary_logloss: 0.58513\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[478]\tTrain's binary_logloss: 0.397716\tValidate's binary_logloss: 0.585127\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[479]\tTrain's binary_logloss: 0.397671\tValidate's binary_logloss: 0.585189\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[480]\tTrain's binary_logloss: 0.397644\tValidate's binary_logloss: 0.585168\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[481]\tTrain's binary_logloss: 0.397595\tValidate's binary_logloss: 0.585166\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[482]\tTrain's binary_logloss: 0.397546\tValidate's binary_logloss: 0.585164\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[483]\tTrain's binary_logloss: 0.397501\tValidate's binary_logloss: 0.585226\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[484]\tTrain's binary_logloss: 0.397183\tValidate's binary_logloss: 0.585211\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[485]\tTrain's binary_logloss: 0.397157\tValidate's binary_logloss: 0.58519\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[486]\tTrain's binary_logloss: 0.397112\tValidate's binary_logloss: 0.585251\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[487]\tTrain's binary_logloss: 0.396799\tValidate's binary_logloss: 0.58524\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[488]\tTrain's binary_logloss: 0.39649\tValidate's binary_logloss: 0.585233\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[489]\tTrain's binary_logloss: 0.396186\tValidate's binary_logloss: 0.58523\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[490]\tTrain's binary_logloss: 0.396141\tValidate's binary_logloss: 0.585289\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[491]\tTrain's binary_logloss: 0.396092\tValidate's binary_logloss: 0.585288\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[492]\tTrain's binary_logloss: 0.395792\tValidate's binary_logloss: 0.585289\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[493]\tTrain's binary_logloss: 0.395496\tValidate's binary_logloss: 0.585294\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[494]\tTrain's binary_logloss: 0.395428\tValidate's binary_logloss: 0.585473\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[495]\tTrain's binary_logloss: 0.395379\tValidate's binary_logloss: 0.585472\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[496]\tTrain's binary_logloss: 0.395087\tValidate's binary_logloss: 0.585481\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[497]\tTrain's binary_logloss: 0.3948\tValidate's binary_logloss: 0.585493\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[498]\tTrain's binary_logloss: 0.394731\tValidate's binary_logloss: 0.585673\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[499]\tTrain's binary_logloss: 0.394683\tValidate's binary_logloss: 0.585671\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[500]\tTrain's binary_logloss: 0.394616\tValidate's binary_logloss: 0.585849\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[501]\tTrain's binary_logloss: 0.394549\tValidate's binary_logloss: 0.586027\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[502]\tTrain's binary_logloss: 0.394265\tValidate's binary_logloss: 0.586043\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[503]\tTrain's binary_logloss: 0.394218\tValidate's binary_logloss: 0.586041\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[504]\tTrain's binary_logloss: 0.393937\tValidate's binary_logloss: 0.586061\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[505]\tTrain's binary_logloss: 0.39366\tValidate's binary_logloss: 0.586085\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[506]\tTrain's binary_logloss: 0.393594\tValidate's binary_logloss: 0.586263\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[507]\tTrain's binary_logloss: 0.393547\tValidate's binary_logloss: 0.58626\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[508]\tTrain's binary_logloss: 0.393482\tValidate's binary_logloss: 0.586437\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[509]\tTrain's binary_logloss: 0.393436\tValidate's binary_logloss: 0.586435\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[510]\tTrain's binary_logloss: 0.393371\tValidate's binary_logloss: 0.586611\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[511]\tTrain's binary_logloss: 0.393097\tValidate's binary_logloss: 0.586638\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[512]\tTrain's binary_logloss: 0.393052\tValidate's binary_logloss: 0.586636\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[513]\tTrain's binary_logloss: 0.392988\tValidate's binary_logloss: 0.586812\n",
      "Early stopping, best iteration is:\n",
      "[463]\tTrain's binary_logloss: 0.399058\tValidate's binary_logloss: 0.584889\n"
     ]
    }
   ],
   "source": [
    "model = lgbm.train(train_set = train_set, \n",
    "                   params = lgbm_params,\n",
    "                   num_boost_round = 1000,\n",
    "                   valid_sets = [train_set, validation_set],\n",
    "                   valid_names = ['Train', 'Validate'], \n",
    "                   evals_result = evaluation_results,\n",
    "                   verbose_eval = 1,\n",
    "                   early_stopping_rounds=50,\n",
    "                   )\n",
    "\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.where(y_pred_proba > 0.5, 1, 0)\n",
    "# saving model\n",
    "# dump(model, 'model_lgbm.joblib')  \n",
    "# dump(evaluation_results, 'evaluation_results_lgbm.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train': OrderedDict([('binary_logloss',\n",
       "               [0.6886078213421075,\n",
       "                0.684164137209993,\n",
       "                0.6828059785264805,\n",
       "                0.6784722061226739,\n",
       "                0.6771580037000169,\n",
       "                0.6729304327641041,\n",
       "                0.6716585376916088,\n",
       "                0.6704121593034365,\n",
       "                0.666302334796062,\n",
       "                0.6650958614274769,\n",
       "                0.6639134733995409,\n",
       "                0.6627546498850793,\n",
       "                0.6616188831810212,\n",
       "                0.6605056783163934,\n",
       "                0.6594145527571926,\n",
       "                0.6583450357778479,\n",
       "                0.6572966683165528,\n",
       "                0.6562690024232056,\n",
       "                0.6552616011995599,\n",
       "                0.6542740382009723,\n",
       "                0.6533058972314862,\n",
       "                0.6523567721867244,\n",
       "                0.6514262664870075,\n",
       "                0.6475760643063317,\n",
       "                0.6466752843019679,\n",
       "                0.6457921155099233,\n",
       "                0.6420402556608777,\n",
       "                0.6411852058387358,\n",
       "                0.640346820296352,\n",
       "                0.6395247573660285,\n",
       "                0.6387186832676477,\n",
       "                0.637928271833941,\n",
       "                0.6371532043122393,\n",
       "                0.636393169055926,\n",
       "                0.6327805848001631,\n",
       "                0.6292400522458899,\n",
       "                0.6285135306781341,\n",
       "                0.6278010511963947,\n",
       "                0.6271023313441693,\n",
       "                0.6264170948485558,\n",
       "                0.6257450716463718,\n",
       "                0.6250859974478619,\n",
       "                0.6216615170178719,\n",
       "                0.6183045565657425,\n",
       "                0.6176745210396454,\n",
       "                0.6170565898421014,\n",
       "                0.6164505227496981,\n",
       "                0.6158560848293296,\n",
       "                0.6152730462246615,\n",
       "                0.6120146300809495,\n",
       "                0.6114500851026389,\n",
       "                0.610896346018377,\n",
       "                0.610353200392471,\n",
       "                0.6071760632299159,\n",
       "                0.6066501419847161,\n",
       "                0.6061342648092837,\n",
       "                0.6056282353424828,\n",
       "                0.605131861273805,\n",
       "                0.6020379505715828,\n",
       "                0.6015573422772473,\n",
       "                0.6010858902940586,\n",
       "                0.6006234166019588,\n",
       "                0.6001697468664072,\n",
       "                0.5971550914926362,\n",
       "                0.5967158599242007,\n",
       "                0.5962849783552577,\n",
       "                0.5958622853059838,\n",
       "                0.5954476226402504,\n",
       "                0.5950408352152314,\n",
       "                0.5946417712703741,\n",
       "                0.5942502820237903,\n",
       "                0.593866221624663,\n",
       "                0.5934894471981669,\n",
       "                0.5931198187322198,\n",
       "                0.5927571990430481,\n",
       "                0.5898457691729617,\n",
       "                0.5894577047444545,\n",
       "                0.5890769652324672,\n",
       "                0.5862276233791285,\n",
       "                0.5858576542728937,\n",
       "                0.58306568345028,\n",
       "                0.5827383717900618,\n",
       "                0.5800027475299785,\n",
       "                0.5796510816631648,\n",
       "                0.5793060375155384,\n",
       "                0.5766277314249516,\n",
       "                0.5739998604397496,\n",
       "                0.5714212627223154,\n",
       "                0.5710921491029357,\n",
       "                0.5707692207753179,\n",
       "                0.570481513603622,\n",
       "                0.5679590606761538,\n",
       "                0.5676490222510361,\n",
       "                0.565175905587908,\n",
       "                0.5648967161450662,\n",
       "                0.5645988551656734,\n",
       "                0.5643282742335036,\n",
       "                0.5640393928917656,\n",
       "                0.5637771136177739,\n",
       "                0.5634969005256764,\n",
       "                0.5632466788201272,\n",
       "                0.5629750093857476,\n",
       "                0.5627269063562547,\n",
       "                0.5624633252560394,\n",
       "                0.5622272409755772,\n",
       "                0.5619895161684305,\n",
       "                0.5617615907840298,\n",
       "                0.5607298455351853,\n",
       "                0.560507196992262,\n",
       "                0.5594976268121157,\n",
       "                0.5585074523713702,\n",
       "                0.5575362841296849,\n",
       "                0.5551614423326283,\n",
       "                0.5542141062911342,\n",
       "                0.553284918357204,\n",
       "                0.5509644698074397,\n",
       "                0.5486866532588686,\n",
       "                0.5477850248335586,\n",
       "                0.5470855793369076,\n",
       "                0.5463994216970521,\n",
       "                0.5457262910934243,\n",
       "                0.5450659323391952,\n",
       "                0.5428512429009071,\n",
       "                0.5421578106821249,\n",
       "                0.5399869702380055,\n",
       "                0.5393103840368707,\n",
       "                0.5371822876425999,\n",
       "                0.5365220739061368,\n",
       "                0.5358744446310915,\n",
       "                0.5352391499185323,\n",
       "                0.5331590357588258,\n",
       "                0.5325390676946147,\n",
       "                0.531974709292852,\n",
       "                0.5299381412508931,\n",
       "                0.5279380087686752,\n",
       "                0.5273463418650567,\n",
       "                0.5267659018725472,\n",
       "                0.5261964681217266,\n",
       "                0.5242400339872739,\n",
       "                0.5223183397161038,\n",
       "                0.5217652298065822,\n",
       "                0.5212225882033809,\n",
       "                0.5206902103221994,\n",
       "                0.5201678958823774,\n",
       "                0.5196554488036628,\n",
       "                0.5191526771221989,\n",
       "                0.5172788344768788,\n",
       "                0.5154379988898924,\n",
       "                0.5136294921380332,\n",
       "                0.5131433633869672,\n",
       "                0.512666396235812,\n",
       "                0.5121984133192422,\n",
       "                0.5117392409695903,\n",
       "                0.509970057156944,\n",
       "                0.5082317314382286,\n",
       "                0.5077854207914346,\n",
       "                0.5073475006727021,\n",
       "                0.5069178095614911,\n",
       "                0.505214753471119,\n",
       "                0.5047950764494271,\n",
       "                0.5043832769045709,\n",
       "                0.5039792036750821,\n",
       "                0.5023102242218174,\n",
       "                0.5019155425936394,\n",
       "                0.5015282585025722,\n",
       "                0.49989089310494905,\n",
       "                0.4995125800810309,\n",
       "                0.4991413508725946,\n",
       "                0.4987770702788274,\n",
       "                0.498419605794438,\n",
       "                0.4980688275648808,\n",
       "                0.49772460829753806,\n",
       "                0.49738682319651584,\n",
       "                0.4970553499668262,\n",
       "                0.49675474254364027,\n",
       "                0.4964349632937482,\n",
       "                0.4961211547684786,\n",
       "                0.4958132038391287,\n",
       "                0.49553404917847427,\n",
       "                0.4952369521321084,\n",
       "                0.4949453968042816,\n",
       "                0.49465927840419127,\n",
       "                0.4944000375891101,\n",
       "                0.49412399507335164,\n",
       "                0.4938530969595134,\n",
       "                0.4935872461489155,\n",
       "                0.49334649272340264,\n",
       "                0.4930899969163567,\n",
       "                0.4915008576114933,\n",
       "                0.4899389742860463,\n",
       "                0.4896891772360321,\n",
       "                0.48815442700949574,\n",
       "                0.48664586790049497,\n",
       "                0.4851629918598083,\n",
       "                0.48370530254772365,\n",
       "                0.4822723153394081,\n",
       "                0.4820312734733818,\n",
       "                0.48179471487250886,\n",
       "                0.48038656068436936,\n",
       "                0.4790021562547144,\n",
       "                0.47877135918755365,\n",
       "                0.478544852043614,\n",
       "                0.478322554076807,\n",
       "                0.4781043860582541,\n",
       "                0.47674418276980596,\n",
       "                0.4766273942969628,\n",
       "                0.4752974850346189,\n",
       "                0.4739898074945368,\n",
       "                0.4727039470436909,\n",
       "                0.47143949822751186,\n",
       "                0.47122911080905927,\n",
       "                0.4699858456343788,\n",
       "                0.46987135342862213,\n",
       "                0.46966601184663603,\n",
       "                0.46955425644928556,\n",
       "                0.4693532935961616,\n",
       "                0.4681311606525221,\n",
       "                0.46802194604249175,\n",
       "                0.46782575348391997,\n",
       "                0.4677191252502742,\n",
       "                0.4675270878084425,\n",
       "                0.467422974056834,\n",
       "                0.467234989692668,\n",
       "                0.4671333207712221,\n",
       "                0.4659320380312042,\n",
       "                0.4647506009147117,\n",
       "                0.46456744818710166,\n",
       "                0.4634055335172348,\n",
       "                0.46330583539183673,\n",
       "                0.46312691832614367,\n",
       "                0.46198421559709935,\n",
       "                0.46188669199103516,\n",
       "                0.4617118800642301,\n",
       "                0.4605880109473123,\n",
       "                0.46049259645441065,\n",
       "                0.46032176369807387,\n",
       "                0.46022854489763826,\n",
       "                0.4600612611212829,\n",
       "                0.45997017857643213,\n",
       "                0.4598063585761829,\n",
       "                0.45870096436401553,\n",
       "                0.4586118074817483,\n",
       "                0.45845166634402135,\n",
       "                0.4583645348066026,\n",
       "                0.45727723488273586,\n",
       "                0.45620770543909095,\n",
       "                0.45605138383127725,\n",
       "                0.4549992220778397,\n",
       "                0.4549135869492196,\n",
       "                0.4538785862913708,\n",
       "                0.4528604184737835,\n",
       "                0.4518587835927071,\n",
       "                0.4517064679225634,\n",
       "                0.450720951538713,\n",
       "                0.4497513872350868,\n",
       "                0.449602164220154,\n",
       "                0.4495178785374413,\n",
       "                0.4493717123573845,\n",
       "                0.448417552497255,\n",
       "                0.4474787890051577,\n",
       "                0.44739604645384545,\n",
       "                0.44647242621564076,\n",
       "                0.446329523357193,\n",
       "                0.4454205927110413,\n",
       "                0.44533934653082147,\n",
       "                0.4451994147372912,\n",
       "                0.44511996584553326,\n",
       "                0.4449828776346273,\n",
       "                0.44490517941681695,\n",
       "                0.4440105102239387,\n",
       "                0.44387623861164827,\n",
       "                0.44380007275348504,\n",
       "                0.4429195586879945,\n",
       "                0.44278802295685027,\n",
       "                0.44271334828974673,\n",
       "                0.4425844629888399,\n",
       "                0.441717632630073,\n",
       "                0.4416444086099787,\n",
       "                0.4415181182215415,\n",
       "                0.4414464827653892,\n",
       "                0.4413227227743359,\n",
       "                0.4404692969954763,\n",
       "                0.44039903008076625,\n",
       "                0.44027775151025217,\n",
       "                0.4402090084433717,\n",
       "                0.4400901630767885,\n",
       "                0.43924126887508586,\n",
       "                0.4384059294918626,\n",
       "                0.4383381889345015,\n",
       "                0.43822150570650276,\n",
       "                0.4381552155597276,\n",
       "                0.43805603005916516,\n",
       "                0.4379910489387099,\n",
       "                0.43789377060481266,\n",
       "                0.4377982971864106,\n",
       "                0.4377045949943249,\n",
       "                0.4376126309588252,\n",
       "                0.43752237264402255,\n",
       "                0.4374587030875789,\n",
       "                0.43663538313892647,\n",
       "                0.43582517044174496,\n",
       "                0.43502783737588085,\n",
       "                0.43493955934048817,\n",
       "                0.43485291766070444,\n",
       "                0.4340679344549851,\n",
       "                0.4332953932527217,\n",
       "                0.43321046838899663,\n",
       "                0.43312711705467044,\n",
       "                0.43304530988209916,\n",
       "                0.4329650179771248,\n",
       "                0.432886213107719,\n",
       "                0.432854011576198,\n",
       "                0.4320932712310915,\n",
       "                0.43134455322550047,\n",
       "                0.43126692231999414,\n",
       "                0.4311907292245082,\n",
       "                0.4311591182027744,\n",
       "                0.4310839846048588,\n",
       "                0.4303469407267531,\n",
       "                0.430315900701253,\n",
       "                0.43024180528698136,\n",
       "                0.43021099251033457,\n",
       "                0.43014599224672734,\n",
       "                0.4294211210910422,\n",
       "                0.4287076561314013,\n",
       "                0.42863459195051984,\n",
       "                0.42856288182964936,\n",
       "                0.42853239859347053,\n",
       "                0.4278640466346859,\n",
       "                0.4277932343668745,\n",
       "                0.42713540895698543,\n",
       "                0.4270658430956428,\n",
       "                0.4270360694017692,\n",
       "                0.4269674013863879,\n",
       "                0.4269378054143291,\n",
       "                0.42687001486470727,\n",
       "                0.42684058944479375,\n",
       "                0.4267736565014504,\n",
       "                0.4267443948170357,\n",
       "                0.42609765154947477,\n",
       "                0.42546132732476943,\n",
       "                0.4253950505627503,\n",
       "                0.4253300049224535,\n",
       "                0.4247033879723533,\n",
       "                0.42463944261557807,\n",
       "                0.4240226384962315,\n",
       "                0.42399436994801926,\n",
       "                0.42393107052316437,\n",
       "                0.4239029274993432,\n",
       "                0.42384037826593685,\n",
       "                0.423233648458556,\n",
       "                0.42263668009596284,\n",
       "                0.4225750225553946,\n",
       "                0.42251451193235434,\n",
       "                0.4219265679572244,\n",
       "                0.4218988616389494,\n",
       "                0.421838886674351,\n",
       "                0.4218112844764106,\n",
       "                0.42123318345142835,\n",
       "                0.421173721968068,\n",
       "                0.42111536778431885,\n",
       "                0.42102584908138235,\n",
       "                0.4204221351025318,\n",
       "                0.4203645821764066,\n",
       "                0.4197699792850653,\n",
       "                0.41918458164441375,\n",
       "                0.41909745106924184,\n",
       "                0.41904054977705857,\n",
       "                0.41901301016853126,\n",
       "                0.41895671877300406,\n",
       "                0.4189292781362283,\n",
       "                0.41884333550710107,\n",
       "                0.41878745892889985,\n",
       "                0.4182119049260247,\n",
       "                0.4176452376277834,\n",
       "                0.4175612354000215,\n",
       "                0.4170037145380269,\n",
       "                0.4169482964447051,\n",
       "                0.4168939116677689,\n",
       "                0.41686655215174634,\n",
       "                0.4163174395321546,\n",
       "                0.41623502872430895,\n",
       "                0.41569475281167084,\n",
       "                0.41516277546070784,\n",
       "                0.4151083257049909,\n",
       "                0.41458425258576675,\n",
       "                0.41406820878843686,\n",
       "                0.41356006107527066,\n",
       "                0.4134800652323036,\n",
       "                0.4134259622945238,\n",
       "                0.4133472395541008,\n",
       "                0.4128473117727367,\n",
       "                0.4127938544087496,\n",
       "                0.41230129841491386,\n",
       "                0.41222419149566986,\n",
       "                0.4117394808702078,\n",
       "                0.4112621493773211,\n",
       "                0.4112089767323288,\n",
       "                0.41073864366676427,\n",
       "                0.41066324321559566,\n",
       "                0.41061067783989796,\n",
       "                0.41055909435921795,\n",
       "                0.4104845846226449,\n",
       "                0.41002150837202617,\n",
       "                0.40997050086935877,\n",
       "                0.40989729850887374,\n",
       "                0.4094412915063729,\n",
       "                0.4093908447110416,\n",
       "                0.4093179892913133,\n",
       "                0.4092910667901672,\n",
       "                0.4092410003688373,\n",
       "                0.4087915519551424,\n",
       "                0.40834890370990656,\n",
       "                0.40827742132601075,\n",
       "                0.4082277084548923,\n",
       "                0.4077917828844607,\n",
       "                0.40772021104471334,\n",
       "                0.4072907391650837,\n",
       "                0.4072414683634264,\n",
       "                0.40717106952818233,\n",
       "                0.407102076637934,\n",
       "                0.4070344614085314,\n",
       "                0.40696819612604657,\n",
       "                0.4069416366677432,\n",
       "                0.4065181453780289,\n",
       "                0.40610102774723567,\n",
       "                0.4060513779194973,\n",
       "                0.4060026540212518,\n",
       "                0.4055912839055819,\n",
       "                0.40552566945736496,\n",
       "                0.40547753754003985,\n",
       "                0.40545094721769814,\n",
       "                0.4050456461780041,\n",
       "                0.4049776565682139,\n",
       "                0.40492955893664023,\n",
       "                0.4048653912656617,\n",
       "                0.4044660675963247,\n",
       "                0.4040727250870827,\n",
       "                0.4040249925818804,\n",
       "                0.4039616520868558,\n",
       "                0.4035737945180527,\n",
       "                0.40351152526465073,\n",
       "                0.40346424269876563,\n",
       "                0.40340313570525144,\n",
       "                0.4030205322772481,\n",
       "                0.40296045055725116,\n",
       "                0.40290156226894475,\n",
       "                0.40252438411555036,\n",
       "                0.402477198281197,\n",
       "                0.40210538292959214,\n",
       "                0.40173910415195474,\n",
       "                0.40169235085617466,\n",
       "                0.4013312605007785,\n",
       "                0.4012668752210774,\n",
       "                0.4012086427015455,\n",
       "                0.40116224068641976,\n",
       "                0.4008064061433907,\n",
       "                0.40045585168339365,\n",
       "                0.4001104926296139,\n",
       "                0.40006427018531243,\n",
       "                0.3997237658169117,\n",
       "                0.3993882963500584,\n",
       "                0.399057781268664,\n",
       "                0.39899537045183464,\n",
       "                0.3989491292190984,\n",
       "                0.3986235055678697,\n",
       "                0.39830267922032553,\n",
       "                0.39824147853555747,\n",
       "                0.3981954433237166,\n",
       "                0.39813739252780417,\n",
       "                0.3980804892673938,\n",
       "                0.39802471066335143,\n",
       "                0.39797920380600715,\n",
       "                0.3979244547509682,\n",
       "                0.39787078741507775,\n",
       "                0.39781818026172117,\n",
       "                0.397766612154498,\n",
       "                0.39771606242745544,\n",
       "                0.3976708330262044,\n",
       "                0.3976444773026707,\n",
       "                0.3975948781728498,\n",
       "                0.39754625814882627,\n",
       "                0.3975011619633482,\n",
       "                0.39718312173666204,\n",
       "                0.39715710833319456,\n",
       "                0.39711213704263315,\n",
       "                0.39679879734707485,\n",
       "                0.396490060632827,\n",
       "                0.3961858537807443,\n",
       "                0.39614099015948717,\n",
       "                0.39609240012648994,\n",
       "                0.3957922510556598,\n",
       "                0.3954964963533671,\n",
       "                0.39542751108559243,\n",
       "                0.39537938032905295,\n",
       "                0.3950874719416962,\n",
       "                0.39479982824161497,\n",
       "                0.3947310888306418,\n",
       "                0.394683408844896,\n",
       "                0.39461574268776606,\n",
       "                0.3945493286578276,\n",
       "                0.39426475827000645,\n",
       "                0.39421755443975415,\n",
       "                0.3939369829824671,\n",
       "                0.39366049940068065,\n",
       "                0.3935938810197565,\n",
       "                0.39354710843120794,\n",
       "                0.39348152346540394,\n",
       "                0.39343552857142705,\n",
       "                0.3933709569508387,\n",
       "                0.39309723642984096,\n",
       "                0.3930518250171437,\n",
       "                0.3929878411020226])]),\n",
       " 'Validate': OrderedDict([('binary_logloss',\n",
       "               [0.6890847372376352,\n",
       "                0.685117511824292,\n",
       "                0.6849705399075716,\n",
       "                0.6811116551732362,\n",
       "                0.6809866910901046,\n",
       "                0.6772324068586388,\n",
       "                0.6771283702214891,\n",
       "                0.677038273794463,\n",
       "                0.673399064916444,\n",
       "                0.6733282521227563,\n",
       "                0.6732703487522879,\n",
       "                0.6732249728504145,\n",
       "                0.6731917528496159,\n",
       "                0.6731703271508399,\n",
       "                0.6731603439964935,\n",
       "                0.673161460937565,\n",
       "                0.6731733446681938,\n",
       "                0.6731956707138084,\n",
       "                0.6732281232549014,\n",
       "                0.6732703947032326,\n",
       "                0.6733221854916783,\n",
       "                0.6733832041149352,\n",
       "                0.6734531663838156,\n",
       "                0.6700606015252213,\n",
       "                0.6701424856132262,\n",
       "                0.670232614752593,\n",
       "                0.6669357704761271,\n",
       "                0.667036779150687,\n",
       "                0.6671453810425902,\n",
       "                0.6672613347877522,\n",
       "                0.6673844052350248,\n",
       "                0.667514363130399,\n",
       "                0.6676509851078323,\n",
       "                0.6677940533580099,\n",
       "                0.6646306342820835,\n",
       "                0.661537891564776,\n",
       "                0.661691045096536,\n",
       "                0.6618499942192836,\n",
       "                0.6620145436142854,\n",
       "                0.662184502845786,\n",
       "                0.6623596862557678,\n",
       "                0.6625399127447689,\n",
       "                0.6595583881219318,\n",
       "                0.6566429270321873,\n",
       "                0.6568304097776564,\n",
       "                0.6570223958350722,\n",
       "                0.6572187225351475,\n",
       "                0.6574192314363699,\n",
       "                0.6576237680340231,\n",
       "                0.6548028145555614,\n",
       "                0.6550119110127829,\n",
       "                0.6552246556200677,\n",
       "                0.6554409073422548,\n",
       "                0.6526982846707459,\n",
       "                0.6529183115796775,\n",
       "                0.6531414996829732,\n",
       "                0.6533677202867612,\n",
       "                0.6535968478755314,\n",
       "                0.6509341172472787,\n",
       "                0.6511661088006485,\n",
       "                0.6514006999884586,\n",
       "                0.651637776249489,\n",
       "                0.6518772258874768,\n",
       "                0.6492905064463075,\n",
       "                0.6495320064850166,\n",
       "                0.6497756070306924,\n",
       "                0.6500212062964075,\n",
       "                0.6502687050350887,\n",
       "                0.6505180063555368,\n",
       "                0.6507690159348815,\n",
       "                0.651021641799516,\n",
       "                0.6512757942488946,\n",
       "                0.6515313858458163,\n",
       "                0.6517883314144143,\n",
       "                0.652046547917551,\n",
       "                0.6495574012381135,\n",
       "                0.6496032153116941,\n",
       "                0.6496519820473317,\n",
       "                0.6472213419196152,\n",
       "                0.6472733485189104,\n",
       "                0.6448975061260273,\n",
       "                0.6451606175532689,\n",
       "                0.6428391608069174,\n",
       "                0.6428962436131047,\n",
       "                0.6429558592249612,\n",
       "                0.6406884098022962,\n",
       "                0.6384698514300357,\n",
       "                0.6362990666683191,\n",
       "                0.6363617846520909,\n",
       "                0.636426786469066,\n",
       "                0.6366934720079601,\n",
       "                0.6345754216645663,\n",
       "                0.6346440853319125,\n",
       "                0.6325731609293578,\n",
       "                0.6324199206348172,\n",
       "                0.6324935467060216,\n",
       "                0.6323458151460842,\n",
       "                0.63242402612537,\n",
       "                0.6322815456714889,\n",
       "                0.6323640947145932,\n",
       "                0.6326389388646606,\n",
       "                0.6327240570846913,\n",
       "                0.6325884430648041,\n",
       "                0.6326774848120182,\n",
       "                0.6329556484088479,\n",
       "                0.6328254367870593,\n",
       "                0.6329816442275953,\n",
       "                0.6327341511752915,\n",
       "                0.6330150314164869,\n",
       "                0.6327822873491831,\n",
       "                0.6325619074031567,\n",
       "                0.6323535811740408,\n",
       "                0.6303761606358694,\n",
       "                0.6301833290067876,\n",
       "                0.630001864536534,\n",
       "                0.6280761969764248,\n",
       "                0.6261916987115652,\n",
       "                0.6260282979430309,\n",
       "                0.6260044045376134,\n",
       "                0.625987584245936,\n",
       "                0.625977641465325,\n",
       "                0.6259743850378102,\n",
       "                0.6241440737205725,\n",
       "                0.624355348655442,\n",
       "                0.6225670086826463,\n",
       "                0.6227836852830763,\n",
       "                0.6210362647508273,\n",
       "                0.6212580715287648,\n",
       "                0.6214837622724929,\n",
       "                0.6217131876401139,\n",
       "                0.6200109605360916,\n",
       "                0.6202448766719767,\n",
       "                0.6202792156143923,\n",
       "                0.6186173419124107,\n",
       "                0.6169907341153344,\n",
       "                0.6172319749503005,\n",
       "                0.6174762935888443,\n",
       "                0.6177235607585931,\n",
       "                0.616138162137371,\n",
       "                0.6145863949197135,\n",
       "                0.6148378166528037,\n",
       "                0.6150918680365514,\n",
       "                0.6153484308937217,\n",
       "                0.6156073899973639,\n",
       "                0.6158686330152705,\n",
       "                0.616132050490841,\n",
       "                0.6146246197062015,\n",
       "                0.6131491452230642,\n",
       "                0.6117049739133226,\n",
       "                0.6119717791848223,\n",
       "                0.6122404602714208,\n",
       "                0.6125109178104995,\n",
       "                0.6127830550043759,\n",
       "                0.611375769380228,\n",
       "                0.6099983650120044,\n",
       "                0.610272696820372,\n",
       "                0.6105484709673815,\n",
       "                0.6108255990718168,\n",
       "                0.6094815196056201,\n",
       "                0.609760132539744,\n",
       "                0.6100399062261921,\n",
       "                0.6103207594557986,\n",
       "                0.6090088949894501,\n",
       "                0.6092908875762997,\n",
       "                0.6095737824045688,\n",
       "                0.608292033862052,\n",
       "                0.6085758462535681,\n",
       "                0.608860393896394,\n",
       "                0.609145606567177,\n",
       "                0.6094314158352474,\n",
       "                0.6097177551576806,\n",
       "                0.6100045596758744,\n",
       "                0.6102917662869729,\n",
       "                0.6105793135504073,\n",
       "                0.6106938791788324,\n",
       "                0.610981550025755,\n",
       "                0.6112693964376779,\n",
       "                0.611557363119484,\n",
       "                0.611677529632685,\n",
       "                0.611965237499974,\n",
       "                0.6122529173536713,\n",
       "                0.6125405194553796,\n",
       "                0.6126653870361929,\n",
       "                0.6129523877385001,\n",
       "                0.6132391777072451,\n",
       "                0.61352571242716,\n",
       "                0.6136544623602312,\n",
       "                0.6139400895498011,\n",
       "                0.6127002211943251,\n",
       "                0.6114867600014471,\n",
       "                0.611771420159756,\n",
       "                0.610584279343092,\n",
       "                0.6094225139860736,\n",
       "                0.6082856309069211,\n",
       "                0.6071731480780018,\n",
       "                0.6060845945859994,\n",
       "                0.6063673918314596,\n",
       "                0.6066496799687044,\n",
       "                0.6055852921788032,\n",
       "                0.6045439095378456,\n",
       "                0.6048250882954558,\n",
       "                0.6051056743237996,\n",
       "                0.6053856343228179,\n",
       "                0.6056649359844296,\n",
       "                0.6046472736471155,\n",
       "                0.6046967898661938,\n",
       "                0.603725439318247,\n",
       "                0.6027753582655055,\n",
       "                0.6018461486472146,\n",
       "                0.6009374209689581,\n",
       "                0.6012147112003516,\n",
       "                0.6003263248055128,\n",
       "                0.6003792357026703,\n",
       "                0.600655676754912,\n",
       "                0.6007092667979963,\n",
       "                0.6009851116936298,\n",
       "                0.6001180329141858,\n",
       "                0.6001727411050618,\n",
       "                0.6004476001397552,\n",
       "                0.6005028785773573,\n",
       "                0.600777032556095,\n",
       "                0.6008328361597589,\n",
       "                0.6011062394115781,\n",
       "                0.6011625250653446,\n",
       "                0.6003173523578788,\n",
       "                0.5994911244608697,\n",
       "                0.5997630008257043,\n",
       "                0.5989554824420601,\n",
       "                0.599013527915768,\n",
       "                0.5992841886838511,\n",
       "                0.5984954880232897,\n",
       "                0.5985543326523151,\n",
       "                0.5988237297204347,\n",
       "                0.5980534675649178,\n",
       "                0.5981130543753462,\n",
       "                0.5983811431914947,\n",
       "                0.5984410088112649,\n",
       "                0.5987081153967528,\n",
       "                0.5987682264598865,\n",
       "                0.5990343184471594,\n",
       "                0.5982830295338529,\n",
       "                0.59834374506641,\n",
       "                0.5986084052521359,\n",
       "                0.5986692953907778,\n",
       "                0.5979360295267987,\n",
       "                0.5972196816911158,\n",
       "                0.5974825016364714,\n",
       "                0.5967827666293164,\n",
       "                0.5968449271049002,\n",
       "                0.5961618566392033,\n",
       "                0.5954947952997022,\n",
       "                0.5948434530958973,\n",
       "                0.5951037294496084,\n",
       "                0.5944678053420727,\n",
       "                0.5938470397039041,\n",
       "                0.5941054900042724,\n",
       "                0.594169501479506,\n",
       "                0.5944267688229442,\n",
       "                0.5938211573139074,\n",
       "                0.5932301642647354,\n",
       "                0.5932949293401205,\n",
       "                0.592718606386436,\n",
       "                0.5929737206954022,\n",
       "                0.5924114456483625,\n",
       "                0.5924769184447723,\n",
       "                0.5927304964430985,\n",
       "                0.5927959201673713,\n",
       "                0.593048240912273,\n",
       "                0.5931135948797652,\n",
       "                0.59256591856929,\n",
       "                0.592816662763092,\n",
       "                0.5928822708318732,\n",
       "                0.5923483870305752,\n",
       "                0.5925975412123723,\n",
       "                0.5926633741620821,\n",
       "                0.5929112167423807,\n",
       "                0.592390782053509,\n",
       "                0.592456812499968,\n",
       "                0.5927030345261516,\n",
       "                0.5927689037717685,\n",
       "                0.5930137849639249,\n",
       "                0.5925067713043711,\n",
       "                0.5925719025779503,\n",
       "                0.5928151424273698,\n",
       "                0.5928802079369138,\n",
       "                0.5931220851871332,\n",
       "                0.5926058470844066,\n",
       "                0.5921025480998117,\n",
       "                0.5921680956369233,\n",
       "                0.5924081681180468,\n",
       "                0.5924736073610292,\n",
       "                0.5925889525581034,\n",
       "                0.5926542596092274,\n",
       "                0.5927697971075828,\n",
       "                0.5928851668619015,\n",
       "                0.593000352833317,\n",
       "                0.5931153394099036,\n",
       "                0.593230111469003,\n",
       "                0.593294900374447,\n",
       "                0.5928031272252734,\n",
       "                0.5923238631567868,\n",
       "                0.5918568840900537,\n",
       "                0.5919694047019063,\n",
       "                0.5920816957965223,\n",
       "                0.5916262388313899,\n",
       "                0.5911826423951895,\n",
       "                0.5912931604732594,\n",
       "                0.5914034339411809,\n",
       "                0.5915134497200236,\n",
       "                0.5916231950773206,\n",
       "                0.5917326577650066,\n",
       "                0.5917631853380303,\n",
       "                0.5913299385660812,\n",
       "                0.5909081553765236,\n",
       "                0.5910151284574252,\n",
       "                0.5911218186575724,\n",
       "                0.5911489459701131,\n",
       "                0.5912546645717457,\n",
       "                0.5908433919685059,\n",
       "                0.5908688868727956,\n",
       "                0.5909729142647422,\n",
       "                0.5909972453240114,\n",
       "                0.5910627466525126,\n",
       "                0.5906625136778585,\n",
       "                0.5902731490760358,\n",
       "                0.5903751003430306,\n",
       "                0.5904767718002053,\n",
       "                0.5904981380290708,\n",
       "                0.5900632413358584,\n",
       "                0.5901629800666306,\n",
       "                0.5897382708908394,\n",
       "                0.5898367229985849,\n",
       "                0.5898545910997115,\n",
       "                0.5899521584844409,\n",
       "                0.589968977387585,\n",
       "                0.5900656692926131,\n",
       "                0.5900814542100855,\n",
       "                0.5901772798566449,\n",
       "                0.590192045951813,\n",
       "                0.5897766142624847,\n",
       "                0.5893713696172765,\n",
       "                0.589464342832451,\n",
       "                0.5895570576694448,\n",
       "                0.5891611260001928,\n",
       "                0.58925258474426,\n",
       "                0.5888661512562393,\n",
       "                0.588875293941035,\n",
       "                0.5889649715518347,\n",
       "                0.5889732036989141,\n",
       "                0.5890621025827926,\n",
       "                0.5886848208446925,\n",
       "                0.5883170449121428,\n",
       "                0.5884037366537703,\n",
       "                0.5884901857644614,\n",
       "                0.5881310801488393,\n",
       "                0.5881345266766412,\n",
       "                0.588219275759009,\n",
       "                0.5882219081000073,\n",
       "                0.5878718129468042,\n",
       "                0.5879548842550454,\n",
       "                0.5880377270907472,\n",
       "                0.5881007298133774,\n",
       "                0.587817154900086,\n",
       "                0.5878994157202467,\n",
       "                0.5876244332987512,\n",
       "                0.5873581226645478,\n",
       "                0.5874210943782893,\n",
       "                0.5875021470727996,\n",
       "                0.587499908079654,\n",
       "                0.5875802946388962,\n",
       "                0.587577345128673,\n",
       "                0.5876402695404858,\n",
       "                0.5877202555642839,\n",
       "                0.5874628327471967,\n",
       "                0.5872137657660371,\n",
       "                0.5872766693453018,\n",
       "                0.5870362588869221,\n",
       "                0.5871144437672653,\n",
       "                0.5871924073881927,\n",
       "                0.5871863993070131,\n",
       "                0.5869536479097948,\n",
       "                0.5870163422059264,\n",
       "                0.5867919312310081,\n",
       "                0.5865752904278008,\n",
       "                0.5866510984827141,\n",
       "                0.5864418892702088,\n",
       "                0.5862401855370495,\n",
       "                0.5860458554646653,\n",
       "                0.5861079268044431,\n",
       "                0.5861820212998217,\n",
       "                0.5862442890624875,\n",
       "                0.58605773266327,\n",
       "                0.5861312719885003,\n",
       "                0.5859516319914987,\n",
       "                0.5860137017163799,\n",
       "                0.5858413792924222,\n",
       "                0.585675910977515,\n",
       "                0.5857477837483331,\n",
       "                0.585588872951314,\n",
       "                0.5856505027544419,\n",
       "                0.5857218293862951,\n",
       "                0.5857929720090088,\n",
       "                0.5858547985343496,\n",
       "                0.5857027601071564,\n",
       "                0.585773351319681,\n",
       "                0.5858351187086135,\n",
       "                0.5856896941719001,\n",
       "                0.5857597316569063,\n",
       "                0.585748806826621,\n",
       "                0.5857344205595769,\n",
       "                0.5858041978076459,\n",
       "                0.5856648724125084,\n",
       "                0.5855318296298689,\n",
       "                0.5855938559213093,\n",
       "                0.5856625623218412,\n",
       "                0.5855357916871212,\n",
       "                0.5855250610942709,\n",
       "                0.5854043923870198,\n",
       "                0.5854721357227267,\n",
       "                0.5854620304175012,\n",
       "                0.585452708169448,\n",
       "                0.5854441476440131,\n",
       "                0.5854363280057397,\n",
       "                0.5854213234501676,\n",
       "                0.585306604767896,\n",
       "                0.5851977507634555,\n",
       "                0.5852649761688559,\n",
       "                0.5853320241373675,\n",
       "                0.5852286332729691,\n",
       "                0.5852212160310514,\n",
       "                0.5852878307681945,\n",
       "                0.5852705226033621,\n",
       "                0.5851726999810718,\n",
       "                0.5852384996721638,\n",
       "                0.5853043397991102,\n",
       "                0.5852971689811841,\n",
       "                0.5852050415077127,\n",
       "                0.5851183936758722,\n",
       "                0.585183348899308,\n",
       "                0.5851765619594852,\n",
       "                0.5850951922309037,\n",
       "                0.5850889661597811,\n",
       "                0.5851537591729307,\n",
       "                0.5851481190934329,\n",
       "                0.5850719695865989,\n",
       "                0.5850668556651535,\n",
       "                0.5850623720957082,\n",
       "                0.5849914898692791,\n",
       "                0.5850558941905483,\n",
       "                0.5849900071225921,\n",
       "                0.5849291625157537,\n",
       "                0.5849925046096671,\n",
       "                0.5849364889025819,\n",
       "                0.585005477565534,\n",
       "                0.5850005579883997,\n",
       "                0.5850636938549524,\n",
       "                0.5850126498819991,\n",
       "                0.5849663933186213,\n",
       "                0.5849248403673236,\n",
       "                0.5849865213269084,\n",
       "                0.5849494776597404,\n",
       "                0.584916979174512,\n",
       "                0.5848889462788502,\n",
       "                0.5849573017314332,\n",
       "                0.5850177249072979,\n",
       "                0.5849941293853985,\n",
       "                0.5849748428131604,\n",
       "                0.5850426935708217,\n",
       "                0.5851022943135861,\n",
       "                0.5850958934227257,\n",
       "                0.5850901156744626,\n",
       "                0.5850849438339748,\n",
       "                0.5851451501093764,\n",
       "                0.5851404892396758,\n",
       "                0.5851364025708414,\n",
       "                0.5851328739784378,\n",
       "                0.5851298876801531,\n",
       "                0.5851274282821299,\n",
       "                0.5851886525807634,\n",
       "                0.5851676837743548,\n",
       "                0.5851657948807707,\n",
       "                0.5851644028291749,\n",
       "                0.5852256614490887,\n",
       "                0.5852106696124744,\n",
       "                0.5851900894946195,\n",
       "                0.5852505471405652,\n",
       "                0.5852396427348544,\n",
       "                0.585232856566983,\n",
       "                0.5852301162873554,\n",
       "                0.5852892604694621,\n",
       "                0.5852877219970338,\n",
       "                0.585288893807036,\n",
       "                0.585293977285714,\n",
       "                0.5854733476267399,\n",
       "                0.5854717168982154,\n",
       "                0.5854806904154641,\n",
       "                0.5854934452503157,\n",
       "                0.5856726625510461,\n",
       "                0.5856709296623668,\n",
       "                0.5858493651651752,\n",
       "                0.5860265661231571,\n",
       "                0.586043127331991,\n",
       "                0.5860410733523257,\n",
       "                0.5860613282412912,\n",
       "                0.5860851834280414,\n",
       "                0.5862625812277953,\n",
       "                0.5862604058646763,\n",
       "                0.5864370280282584,\n",
       "                0.5864349868584786,\n",
       "                0.5866108308366487,\n",
       "                0.5866383583382848,\n",
       "                0.5866363081851765,\n",
       "                0.5868117073055642])])}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD6CAYAAACvZ4z8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAA830lEQVR4nO3dd3hVVfbw8e9K770QSCCUUAKEltCRJk0cEFGkOIBYGds4Y59REXVG5+c4jr7O2CsoIiqCZSgqRakBQgm9E2oSICSE9P3+cS4xYkgupNzksj7Pcx/u2afctWNcd2efffYWYwxKKaWcl4ujA1BKKVWzNNErpZST00SvlFJOThO9Uko5OU30Sinl5DTRK6WUk7Mr0YvIUBHZISK7ReTRcvb/S0RSbK+dInK6zL5JIrLL9ppUjbErpZSyg1Q2jl5EXIGdwCAgDVgLjDPGbL3I8fcCnYwxU0QkBEgGEgEDrAO6GGNOXezzwsLCTGxs7GVURSmlrlzr1q3LMMaEl7fPzY7zuwK7jTF7AURkFjASKDfRA+OAp2zvhwCLjDEnbecuAoYCn1zsw2JjY0lOTrYjLKWUUueJyIGL7bOn66YRcKjMdpqtrLwPagI0BX641HOVUkrVjOq+GTsWmGOMKb6Uk0TkDhFJFpHk9PT0ag5JKaWubPYk+sNATJntaFtZecby624Zu841xrxpjEk0xiSGh5fbxaSUUuoy2dNHvxaIE5GmWEl6LDD+woNEpDUQDKwsU7wA+JuIBNu2BwOPVSlipeqJwsJC0tLSyMvLc3Qoyol4eXkRHR2Nu7u73edUmuiNMUUicg9W0nYF3jXGpIrIdCDZGDPPduhYYJYpM4zHGHNSRJ7B+rIAmH7+xqxSzi4tLQ1/f39iY2MREUeHo5yAMYbMzEzS0tJo2rSp3efZ06LHGPMt8O0FZU9esD3tIue+C7xrd0RKOYm8vDxN8qpaiQihoaFc6r1MfTJWqRqkSV5Vt8v5nXKaRF989iQZX0/n1O61lR+slFJXEKdJ9CdyighZ+xL7V37u6FCUUqpOcZpE3yAinD0Sg/fxdY4ORak6ITMzk44dO9KxY0caNGhAo0aNSrcLCgoqPDc5OZn77rvvkj7Pz8+v3PInn3ySxYsXX9K1qsOSJUu49tprL/v8i9WnPrLrZmx9ICKk+bWnf843kDoX2l7n6JCUcqjQ0FBSUlIAmDZtGn5+fjz44IOl+4uKinBzKz8FJCYmkpiYWC1xTJ8+vVquU1G8qmJO9VNLazEeUr4h9+fX8dFEr+qQp+ensvXImWq9ZnzDAJ76XdtLOmfy5Ml4eXmxYcMGevXqxdixY7n//vvJy8vD29ub9957j1atWrFkyRJefPFFvv76a6ZNm8bBgwfZu3cvBw8e5I9//ONFW/sPPPAACxcupEGDBsyaNYvw8HAmT57Mtddeyw033EBsbCyTJk1i/vz5FBYW8tlnn9G6dWvWrFlTbhzvv/8+X3zxBTk5ORQXF9OkSROuv/56rrvuOgAmTJjAmDFjGDlyZIX1PnnyJFOmTGHv3r34+Pjw5ptvkpCQQHp6OuPHj+fIkSP06NGDRYsWsW7dOsLCwkrPNcbw8MMP89133yEi/PWvf+Wmm27i6NGj3HTTTZw5c4aioiL++9//0rNnT2699VaSk5MREaZMmcIDDzxwSf+NaoLTdN0AXHP1IGYyDJ8jK6HgrKPDUapOSktLY8WKFbz00ku0bt2a5cuXs2HDBqZPn87jjz9e7jnbt29nwYIFrFmzhqeffprCwsLfHHP27FkSExNJTU2lb9++PP300+VeKywsjPXr1zN16lRefPFFgArjWL9+PXPmzGHp0qXceuutvP/++wBkZWWxYsUKhg8fXmmdn3rqKTp16sSmTZv429/+xsSJEwF4+umnGTBgAKmpqdxwww0cPHjwN+d+8cUXpKSksHHjRhYvXsxDDz3E0aNH+fjjjxkyZEjpvo4dO5KSksLhw4fZsmULmzdv5pZbbqk0ttrgVC36UD9PsiK6wYnv4MORcFvt9wsqVZ5LbXnXpBtvvBFXV1fASpaTJk1i165diEi5CRxg+PDheHp64unpSUREBMePHyc6OvpXx7i4uHDTTTcBcPPNN3P99deXe63z5V26dOGLL76oNI5BgwYREhICQN++ffnDH/5Aeno6n3/+OaNHj7arO+enn37i88+tgRoDBgwgMzOTM2fO8NNPP/Hll18CMHToUIKDg8s9d9y4cbi6uhIZGUnfvn1Zu3YtSUlJTJkyhcLCQq677jo6duxIs2bN2Lt3L/feey/Dhw9n8ODBlcZWG5yqRQ9Q0nIYP5W0wxzZoK16pcrh6+tb+v6JJ56gf//+bNmyhfnz5190ugZPT8/S966urhQVFVX6ORcb733+WmWvU1EcZeMFmDhxIjNmzOC9995jypQplcZRU6666iqWLVtGo0aNmDx5Mh9++CHBwcFs3LiRfv368frrr3Pbbbc5LL6ynC7RJzUL5+2iYUhJESz4C5zc6+iQlKqzsrKyaNTImjn8fJfI5SopKWHOnDkAfPzxx/Tu3btG4pg8eTIvv/wyAPHx8XZdv0+fPsycOROwRuOEhYUREBBAr169mD17NgALFy7k1KnfronUp08fPv30U4qLi0lPT2fZsmV07dqVAwcOEBkZye23385tt93G+vXrycjIoKSkhNGjR/Pss8+yfv16O38CNcvpEn23ZqF4NOvNPomBde/DD886OiSl6qyHH36Yxx57jE6dOtnVSq+Ir68va9asoV27dvzwww88+eSTlZ90GXFERkbSpk2bS+r/njZtGuvWrSMhIYFHH32UDz74ALD67hcuXEi7du347LPPaNCgAf7+/r86d9SoUSQkJNChQwcGDBjAP/7xDxo0aMCSJUvo0KEDnTp14tNPP+X+++/n8OHD9OvXj44dO3LzzTfz97//3e4Ya1KlSwnWtsTERFPVFabeWraX577dxvaOc/BKWwF/3g7l/RlpDBTmgofvb/cpVUXbtm2jTZs2jg7D6eTm5tK+fXvWr19PYGBgla6Vn5+Pq6srbm5urFy5kqlTp5YOSa3LyvvdEpF1xphyx8Q61c3Y8zrEBAGwx6cDbXO+gOmh4OkH7j5QUgwlhVBcBIVnwZTAsP+Dbnc4NmilVKUWL17MrbfeygMPPFDlJA9w8OBBxowZQ0lJCR4eHrz11lvVEGXd45SJPiE6kIaBXjy4oxVfDXwGj4LTkJ9ttd5d3MDFHVzdrcS/eTZs/1oTvVL1wNVXX82BA79eGnXBggU88sgjvypr2rRp6WiaisTFxbFhw4ZqjbEucspE7+Xuyj/HdGT826v4f+eG8KfBrS5+cFEerPoPvNweJsyB8AqOVUrVOUOGDGHIkCGODqNOc7qbsef1aB5K58bBrNiTWfGBnSdB/HVw+hBs/coqy8uCIylwbEtNh6mUUjXOKVv053WKCeKjVQcoLC7B3fUi32nhLeHG9+DkHljxKqz6L5yzLYLl4gZ/2g5+uo6tUqr+ctoWPUBibAj5RSXMWnuo8oN73geNukD8CLj6aRjyNygpgj3f13ygSilVg5w60Q+Oj6Rvy3Cemb+VTWmnKz64/Q0wcS787t/Q+4/QbSr4hsOuRbUQqVLVr3///ixYsOBXZS+//DJTp0696Dn9+vXj/PDma665htOnT//mmGnTppXOUXMxc+fOZevWrZcU78Wue+TIEW644YZLulZ1KfvzuFT2/Jxqi1MnehcX4eWbOhLu78nUGes5nVvxHNwXnAzNB1ot+q1fWQ9efXwTLP9nzQWsVDUaN24cs2bN+lXZrFmzGDdunF3nf/vttwQFBV3WZ19Oor+Yhg0blj5xW1XFxcXVcp36xqkTPUCwrwevTejMiew8Hvg0hZKSS3hArOVgOHcKZk+E5S/BwVWw7EUoLH8+EKXqkhtuuIFvvvmmdJGR/fv3c+TIEfr06cPUqVNJTEykbdu2PPXUU+WeHxsbS0ZGBgDPPfccLVu2pHfv3uzYsaP0mLfeeoukpCQ6dOjA6NGjyc3NZcWKFcybN4+HHnqIjh07smfPHvbs2cPQoUPp0qULffr0Yfv27eV+5saNG+nRowdxcXGlY9r3799Pu3btAGt6hOuvv56hQ4cSFxfHww8/XHruxeoUGxvLI488QufOnXn++efp3Llz6b5du3b9arsin3zyCe3bt6ddu3a/Gs75zjvv0LJlS7p27crtt9/OPffc85tzU1JS6N69OwkJCYwaNap0qoVXXnmF+Ph4EhISGDt2LABLly4tXSCmU6dOZGdn2xVfRZz6Zux5HWOCePLaeJ74KpV/LNjBo8Na23di/HUwxh0CoyGiDez/GWaOhv3LIW5QjcasnMx3j8KxzdV7zQbtYdjzF90dEhJC165d+e677xg5ciSzZs1izJgxiAjPPfccISEhFBcXM3DgQDZt2kRCQkK511m3bh2zZs0iJSWFoqIiOnfuTJcuXQBrJsrbb78dgL/+9a+888473HvvvYwYMaJ0DnqAgQMH8vrrrxMXF8fq1av5wx/+wA8//PCbz9q0aROrVq3i7NmzdOrUqdwpiFNSUtiwYQOenp60atWKe++9l5iYmArrFBoaWjrvzOLFi0lJSaFjx4689957dk2lcOTIER555BHWrVtHcHAwgwcPZu7cuXTt2pVnnnmG9evX4+/vz4ABA+jQocNvzp84cSKvvvoqffv25cknn+Tpp5/m5Zdf5vnnn2ffvn14enqWdpO9+OKLvPbaa/Tq1YucnBy8vLwqja8yTt+iP+/m7k2Y0K0xry/dw2x7bs4CuLhaN2cbdQZ3b4jtDe6+sP0byNgNKZ/8MiRTqTqobPdN2W6b2bNn07lzZzp16kRqamqF3SzLly9n1KhR+Pj4EBAQwIgRI0r3bdmyhT59+tC+fXtmzpxJamrqb87PyclhxYoV3HjjjXTs2JE777yTo0ePlvtZI0eOxNvbm7CwMPr378+aNWt+c8zAgQMJDAzEy8uL+Pj40geoKqrT+emTAW677Tbee+89iouL+fTTTxk/fnxFP0IA1q5dS79+/QgPD8fNzY0JEyawbNky1qxZQ9++fQkJCcHd3Z0bb7zxN+dmZWVx+vRp+vbtC8CkSZNYtmwZAAkJCUyYMIEZM2aUTrfcq1cv/vSnP/HKK69w+vTpallV64po0YM1Zeq0EW05eDKXx7/cTHSINz2bh1V+YlnuXtC8P6x7z3oBiAs8uAu8Q6x+faXKU0HLuyaNHDmSBx54gPXr15Obm0uXLl3Yt28fL774ImvXriU4OJjJkydfdHriykyePJm5c+fSoUMH3n//fZYsWfKbY0pKSggKCrJrDpkLpzYub6rj8qZMrqxOZac6Hj16dOmCI126dCE0NNSOmtaMb775hmXLljF//nyee+45Nm/ezKOPPsrw4cP59ttv6dWrFwsWLKB1azt7IS7iispM7q4u/L/xnWka5svUGevZefwy+r6uehCSboMRr8KNH1hz5bxxFTwbDvuWVX/QSlWBn58f/fv3Z8qUKaWt+TNnzuDr60tgYCDHjx/nu+++q/AaV111FXPnzuXcuXNkZ2czf/780n3Z2dlERUVRWFhYOg0wgL+/f2nfckBAAE2bNuWzzz4DrKX5Nm7cWO5nffXVV+Tl5ZGZmcmSJUtISkqyq56XUicvLy+GDBnC1KlT7Z4Bs2vXrixdupSMjAyKi4v55JNP6Nu3L0lJSSxdupRTp05RVFRUurhJWYGBgQQHB7N8+XIAPvroI/r27UtJSQmHDh2if//+vPDCC2RlZZGTk8OePXto3749jzzyCElJSRe9n3Ep7GrRi8hQ4N+AK/C2MeY3zRMRGQNMAwyw0Rgz3lZeDJzvnDxojBlx4bm1KdDbnXcnJzH6vyv4/TurmXNXT2JCfOy/QMNO1gus2S9bDYeCHGsunWX/B1vnwYltMOIVCG1eM5VQ6hKMGzeOUaNGlXbhnJ9at3Xr1sTExNCrV68Kz+/cuTM33XQTHTp0ICIi4lfJ95lnnqFbt26Eh4fTrVu30uQ+duxYbr/9dl555RXmzJnDzJkzmTp1Ks8++yyFhYWMHTu23L7shIQE+vfvT0ZGBk888QQNGzZk//79ldbxUus0YcIEvvzyS7tXgIqKiuL555+nf//+GGMYPnx46Tq1jz/+OF27diUkJITWrVuXO9naBx98wF133UVubi7NmjUr7Tq6+eabycrKwhjDfffdR1BQEE888QQ//vgjLi4utG3blmHDhtkVY0UqnaZYRFyBncAgIA1YC4wzxmwtc0wcMBsYYIw5JSIRxpgTtn05xhg/ewOqjmmK7bHjWDZj3lhJkI87c+7qSbi/Z+UnVeSru2HDDGuitKI86DAeojrAia0w4Anwtf15WFICR1Ng9/dwaDUM+Cs07FjV6qg6SKcprrtefPFFsrKyeOaZZ6p8rZycHPz8/CgqKmLUqFFMmTKFUaNGVUOUF1cT0xR3BXYbY/baLjYLGAmUvXtzO/CaMeYUwPkkX5e1auDPe7ckcdMbK3n7p708NqyK/0MO+Tsk3mqNhPhkLKTMsF4A3kEQ3gZ2L7bG5efa5t8RV6vff/BzENykap+vlLLLqFGj2LNnT7mjfi7HtGnTWLx4MXl5eQwePJjrrruuWq5bnexJ9I2AssNU0oBuFxzTEkBEfsbq3plmjPmfbZ+XiCQDRcDzxpi5F36AiNwB3AHQuHHjS4m/Sjo3DiYhOoi1+05W/WJeAdboHIBh/7DG3DfpCZ/fBj/9yyr3CYMWV1uvZv1hyd8g+V3YNh/uWPJLl5BSqsaUN33xqFGj2Ldv36/KXnjhBbtmxawrT79WpLpG3bgBcUA/IBpYJiLtjTGngSbGmMMi0gz4QUQ2G2P2lD3ZGPMm8CZYXTfVFJNdEpsE8+7P+zh+Jo/IgKqPVwWsvvnz/fPX/AP2LYdmfaFBh1+PzBnwBMR0g/n3w4wb4K7lENCwemJQStnNnrnr6zN7Rt0cBmLKbEfbyspKA+YZYwqNMfuw+vTjAIwxh23/7gWWAHWq2TqqcyM8XF0Y9+YqsnILq/8DGnWx5s5p2Om3wy99QqDDWEicArkZsPCvUHQJ0zSoOq+uLdWp6r/L+Z2yJ9GvBeJEpKmIeABjgXkXHDMXqzWPiIRhdeXsFZFgEfEsU96LX/ftO1zrBgG88ftE9macZUHqMccEMfTvED8StnwOC//imBhUtfPy8iIzM1OTvao2xhgyMzMv+WnZSrtujDFFInIPsACr//1dY0yqiEwHko0x82z7BovIVqAYeMgYkykiPYE3RKQE60vl+bKjdeqKXi1CCfZxZ83+k4xJiqn8hJow6BlrpswNM+HMERg0XYdn1nPR0dGkpaWRnp7u6FCUE/Hy8iI6OvqSzql0eGVtq63hlRe686NkVu87yad39KBVA/9a/3wAjm6EL+6AzD3QaYI1ZbJSStmhouGVV9STsRV5aEgrPN1cGPPGStYdOOWYIKI6wN2rIWEMbJ5jPYSllFJVpInepkWEP3Pu6kmwjzs3v72aJTsc+ChAl8nW07abq2cObqXUlU0TfRkxIT58dldPYsN8ue2DZPtnuaxu0UkQ0RbWvAXrP4QtXzgmDqWUU9BEf4Fwf09m39mdbs1CePSLTWTm5Nd+ECKQdCucSIV598KcWyBtXe3HoZRyCproy+Hv5c6fBrWixMCa6nhq9nJ0uQXuWAq/n2ttvz3AuklbnpISKC60XiUltRaiUqp+uGLmo79UCdGBeLu7smpvJsPaR9V+AC4uv0x2dsN7Vqv+/eHWA1huXtbEaSe2gos7ZO6ypktGrKkYmvS25j8Pqr3pJJRSdZcm+otwd3UhMTaYVXsd1KIvq9311kRoG2ZAxi4rybu4QYMEqxUfNwhcPaAoHwqyrZu4/3sMRv4/8A52dPRKKQfTRF+B7s1C+b8FOziadY6oQG/HBtP1dutlD59QayK1PT/CbYshMr5mY1NK1WnaR1+BwfGReLi5MPo/K0g5dNrR4dhvwJMw8Svrpu5PLzk6GqWUg2mir0BcpD+f39UTEWHM6yuZsepA/Zi3xMUFmvWzxuNv+QJ+fgXWfQCbZkPh5a0NqpSqv3QKBDucOlvAHz9NYenOdO7q25xHh1Vtod5ac+YovDMIsso8D9ByGIx+CzwdNM2DUqpG6BQIVRTs68G7k5MY2rYBM1cdoKi4ngxhDIiCP26Gh/fBA6nQcijs/A5mjIbcOnCTWSlVKzTR28nVRbi2QxTZ+UVsOpzl6HDsJ2LNex8YDSP/A417WGvVfjjCGp2z+g0de6+Uk9NEfwl6NLMW+F6xO8PBkVwm31C4+QtoMwKObYbPb4XvHoa3+lvTIyulnJIm+ksQ6udJm6gAft6d6ehQLp+HD9z4gfUQ1m3fWytfHU2Br/4Ax1MdHZ1SqgZoor9E/VqFs2pfJs99s5W8wmJHh3N5XFysh7CiE2HKQvjzTvAKhLlT4cPr4Mu7oI7dpFdKXT5N9Jfo3gEtGN+1MW8t38e1r/7EprTTjg6patw8wD8S+j5iLXySsQs2fgLb5lnvV75mLVy+/iNHR6qUukw6vPIyLd2ZziNzNnGusJi1f7kaD7d6/p1pDOSfAXdfeL03ZOwEY/uLxTPAmmrhnrUQVANLLZ47DUfWQ2R78Auv/usrdQXQ4ZU1oG/LcKaPbEvWuULWH3TQilTVScTqvnF1s+bIiR8Bw/8J92+CqT9bxyx4zJpBc+V/YPu3l/9ZxsCxLbD8n/DOEPhHU/hoFMwcbc3Xo5SqVtqir4LsvEI6TV9Eu0aBvHhjB1pE+Dk6pJqz/J/w/fRftl09rS8AF1fw8AO/iIrPzz1pLX6+8zvY/xOctS2YHdUB4gaDVxAs/ItVNmGONVGbUspuFbXoNdFX0Vcph3li7hbOFRbz+dSeJEQHOTqkmlFUAN8/DYEx1gpYM0ZZ3TmFudZ0yJHt4eRe6P+YNWNm06us7e3fwo7v4OBKqyvIr4E1PUNsb2hxtfVQ13lL/w9+fBY8A+GOHyG0ucOqq1R9o4m+hp3IzqP3Cz8ysXsT/nrtFTJT5NZ5sOEjaNAefv63lcDPnYLCsyCuENoCMnZYx0a2g1bDrFdUJ2vUz8Wc2g9v9LUe8Lp1kTUcVClVKU30teD376xm94kc3pqYSLtGgY4Op3adO2V1vaQlW/PqrHjVmkun1TXQaigEx17a9XYthpk3QNtR0KwvNO0LIU1rInKlnIYm+lqwYk8G93y8gVO5BdyUGMO0EW3xcnd1dFj119J/wI/PWe+DmsCQv8Ge760W/9XTrL59pVSpKo+6EZGhIrJDRHaLyKMXOWaMiGwVkVQR+bhM+SQR2WV7Tbq8KtR9PZuH8eOD/ZjcM5ZZaw/x5YbDjg6pfuvzIIz5yHrlHIdPJ8DGT+HACnjjKuuLQClll0pXmBIRV+A1YBCQBqwVkXnGmK1ljokDHgN6GWNOiUiErTwEeApIBAywznauE4xH/K1Ab3eevDaeBVuOsXjrccYmxSAijg6rfnJxsYZ4AgQ0hLMZVjfO6UOw6Amrte/hBz3+4Ng4laoH7FlKsCuw2xizF0BEZgEjga1ljrkdeO18AjfGnLCVDwEWGWNO2s5dBAwFPqme8OseEWF4QhRvLd/Hdf9ZwSNDWtGzRZijw6rfosv8NRreEm6aaS2WvuAxWPMmdJ9qPXR19gS0vhZyTsCxTdbEbV6B8Lt/WzN4KnWFsifRNwLKrFxBGtDtgmNaAojIz4ArMM0Y87+LnNvosqOtJx4Z2prm4X78+/tdjH97NbPv7EHXpppoqo2rG4x+B+bdC5tmWTNwnrf2betfNy/rJvD+n2DbfGg32hoh1Lg7eIdYQzfFxXoALHO39WViSsDD13op5USqa3FwNyAO6AdEA8tEpL29J4vIHcAdAI0bN66mkBzHzdWFsV0b87sODeny7CK+3XxUE311c/OA69+wnt49sc1aAD3nuDXyJ7IthMZZx+xbDouehC1zrNd5AY2sh7iKzv36ut7BcNXDED8SAp2+TaKuEPYk+sNA2QlOom1lZaUBq40xhcA+EdmJlfgPYyX/sucuufADjDFvAm+CNerGztjrPF9PN3q3COeTNQcBuK1PU6KDdVx4tfL0g5gk631IM+tVVtM+1sNXOScgPxt2L4aT++DEVmjd2vpScPWw5trxi4BNn1ldQgseg9g+1nQQlzo8VKk6ptLhlSLiBuwEBmIl7rXAeGNMapljhgLjjDGTRCQM2AB0xHYDFuhsO3Q90OV8n3156uvwyos5mnWOfy7cydwNhzHA7xKiuLNvc9pEBTg6NFWegrPWl8GeH2HL59bTv7G94aoHwT8Kso9CdNeKH/pSygGqPI5eRK4BXsbqf3/XGPOciEwHko0x88QaWvJPrButxcBzxphZtnOnAI/bLvWcMea9ij7L2RL9eUdOn+Odn/bxyZqD5BYU061pCBN7xHJN+wY6MqeuOrUfvv4T7FsGJYW/lDdKtJ7ybdTZep++A06kgl+k1b8f3gZyM6x7AD5hkL4NXNzAJ9TqZvLwgeYDrcndcjMh+wicOQJZaZB9DGK6WXP96O+FugT6wFQdcjq3gFlrDzFj1QHSTp3j79e3Z1zX+n9fwqnlnYHkd6wRPOdOWXP051ZxlbGgJtY9haK88vdHJ1n3C2K6QrP+1hfBmSOQd9p2/yDGOt8zAHzDrOu4uFs3qtUVSRN9HVRcYhj8r6U0CPRi5m3dHR2OulTHtkDaGshJt/r5w+Ks4Zyu7nB8qzXRW84xq1Uf3tpqqbu6Q1grOLIBdi20Rv6ENLO6hAIaWvP7eAdbcwitftNq0advrzwWVw8oLrBGE7UeDoXnrOQf2c66/rlT1is/2zqu1TXWfQcXN+2CciKa6Ouolxbu4JUfdpMUG8y7k5Pw93J3dEiqrjm22erSCWhojRRCYOPHUFIEvhFW91JxAXgFQNo6OPCz9T4rzRouWhHvEAhvZV23IMfqWmo93OpSimhjfUZgtDVJnSnWYad1XEWJXv/Oc6C7B7TA19ONv3+3ne+2HGNMYg2s3qTqtwbtrVdZPe+t/Lz8HOuvgbws62Ex7xDrC6Ao31oqsiAXTu6BE9utvzY8/a3pJVJmXnAhAYz1l4l3MMR0B3dvazWyvDPWXwmhza2/aPyjrGmrvQKt2Uwj4617FDoDacWMsYb65mZas7827FTtH6EtegczxtD7hR8B+PLunkT4ezk4InXFys+x/oIoKYLTB6w+//TtVku+MBfOHLXWFQDrS8MzwNp3bDOcucjcTi5uVheSi5vtSyDI+sLwC7e+DDz9rfsOeWesobJF+VZZUZ6VAAOjrb9Y3H3A9/wykwbcvK3P9vSzvrTcva1ysM4zxdY5bp62exfuv9zDcHG3urtcbJMOFuVZo6vcPK3y8zfBi4usfUV51s/EGOszTEmZ97btkiIr9pKiXx+Tm2l9GRblWSO2zt90L8q3utNyjluL8Jz/66thJ7hjyWX959MWfR0mIjx+TRvu/ng97/+8n4eHtnZ0SOpK5ekHTXrYNvpc2rnFhdZ8RB6+VuLKPmqNRspKs55RQKxEnZdlLUhz4CfrPVj7PP2t7iNXT+shNlcP23ULqqlyFyEuv+3icnG3Jeri6v8872Dr/o2bt/XcRoME8I+0fjbewdb9nhqgib4OGJ4QxSdrwvho5QFuTIyhaZj2hap6xtX9l9XCvAKs7pzY3hc/3hgoKYaCbGtFMRcXWyvZts/FBUpKrPmL3Lys5xvOLz8pAoV51hdDfrY1uV1hrpW0z7fGxdX6wijMs4bGFhdare3iwl+2iwt/mfbC1d36UikqgOJ861puXraXp/UXiYhVjtg+R375TFcP218NbrYy23FegVYCd3UD/4bg7pi/2DXR1xF/6Nec8W+v5rlvtvH2pHL/+lLKeYhYyc87+NdlZf91cQH/BtZ77yCdkqIKdGxVHdGzRRi39Ipl2a50FqQeo67dO1FK1V+a6OuQm7s3IdTXgzs/WsfcFF24RClVPTTR1yHNw/1Y/nB/Gof48MV6TfRKqeqhib6OcXN1YXhCFCv2ZJKZk+/ocJRSTkATfR10bUIUxSWG/6Uec3QoSiknoIm+DoqPCqBZmC9fbzzq6FCUUk5AE30dJCJcmxDFqn2ZPDxnI1sOZ1V+klJKXYQm+jrq1t7NuL5TNN9sOsrYN1eRW1Dk6JCUUvWUJvo6KtDHnX+O6cDbk5LIyS/iX4t2OjokpVQ9pYm+juvWNIROjYN4a/k+1h045ehwlFL1kCb6Os7FRfhwSlc83Vx45uutfLkhjVNna3iiJ6WUU9G5buoBfy93Hr+mDa/+sJsHPt2Ii0CXJsEMaB3JwDYRxEX46bqzSqmL0vno65GSEsPmw1l8v+04328/QeqRMwDEhHjTv1UEU/s1JyrQ28FRKqUcQZcSdFJHs87xw/YTfL/tBMt2phPq58HTI9oxtF0DR4emlKplFSV67aOvx6ICvZnQrQnvTk5i1h3d8fV0456P17N0Z7qjQ1NK1SGa6J1EYmwIX93di7hIf6bOWMeOY9mODkkpVUdoonci/l7ufHBLEgK8vXyvo8NRStURdiV6ERkqIjtEZLeIPFrO/skiki4iKbbXbWX2FZcpn1edwavfigjwYnhCFPM3HWFzmk6doJSyI9GLiCvwGjAMiAfGiUh8OYd+aozpaHu9Xab8XJnyEdUTtqrInwa1ItTXk4nvruZEdp6jw1FKOZg9LfquwG5jzF5jTAEwCxhZs2GpqmgQ6MW7k5M4lVvIZ8lpjg5HKeVg9iT6RsChMttptrILjRaRTSIyR0RiypR7iUiyiKwSkeuqEKu6BK0a+NO1aQifJR/S9WeVusJV183Y+UCsMSYBWAR8UGZfE9vYzvHAyyLS/MKTReQO25dBcnq6Dg2sLjclxrA/M5fvtugCJkpdyexJ9IeBsi30aFtZKWNMpjHm/Lp3bwNdyuw7bPt3L7AE6HThBxhj3jTGJBpjEsPDwy+pAurihidEER8VwH2fbGDWmoPaslfqCmVPol8LxIlIUxHxAMYCvxo9IyJRZTZHANts5cEi4ml7Hwb0ArZWR+Cqcl7ursy6szvdm4Xy6BebefCzTeQVFjs6LKVULas00RtjioB7gAVYCXy2MSZVRKaLyPlRNPeJSKqIbATuAybbytsAybbyH4HnjTGa6GtRgJc7H0zpyv0D4/h8fRofrTzg6JCUUrVM57q5gtzw3xWknTrH53/oSaMgnfxMKWeic90oAB4d1pqsc4X87Zttjg5FKVWLNNFfQRJjQ7i5e2O+23KUJTtOODocpVQt0UR/hbmzb3Oahvny2BebKSoucXQ4SqlaoIn+ChPm58kjQ1tzNCuPx7/czLkCHYWjlLPTRH8FGhQfyR/6NeezdWlc99rP5BYUOTokpVQN0kR/BRIRHh7amv9O6MKO49nMWafz4SjlzDTRX8GGtI2kc+Mgps1L5bUfdzs6HKVUDdFEfwUTEd6f0pVB8ZG8uHAHBzNzHR2SUqoGaKK/wgV4ufP0iHa4ivDein2ODkcpVQM00SsaBHpxbUIUn649xMJUnelSKWejiV4B8OfBrWgc4sMdH63jya+2ODocpVQ10kSvAIgJ8WH+vb0Z360xH648wPZjZxwdklKqmmiiV6XcXV14eEgrfDxceXPZXkeHo5SqJpro1a8E+XhwU1IMX244zF/nbuZ0boGjQ1JKVZGbowNQdc+Dg1thDHy4cj/fbDrKg0NaMTapMa4u4ujQlFKXQVv06jd8Pd2YNqIt39zXh7hIf/7y5Raenp/q6LCUUpdJE726qDZRAXx6R3fGJsXwyZqDHDl9ztEhKaUugyZ6VSER4d6BcRiD3qBVqp7SRK8q1SjIm9Gdo7VVr1Q9pYle2WVqv+YYYPC/lvHvxbvIydepjZWqLzTRK7vEhvny7X296dUilH8t3snEd1ZT1xaWV0qVTxO9sluLCH/e+H0iz17XjvUHT/P9Nl13Vqn6QBO9umRjk2JoEurD377dxroDJx0djlKqEpro1SVzc3Xh2evakXWukNH/XamToClVx2miV5elT1w4yx/pz9ikGD5ceYD3ft6nC40rVUfZlehFZKiI7BCR3SLyaDn7J4tIuoik2F63ldk3SUR22V6TqjN45Vg+Hm48PrwNSbHBPD1/K/1e/JHjZ/IcHZZS6gKVJnoRcQVeA4YB8cA4EYkv59BPjTEdba+3beeGAE8B3YCuwFMiElxt0SuHC/ByZ/adPZh5Wzcycwq4f9YGth7RKY6VqkvsadF3BXYbY/YaYwqAWcBIO68/BFhkjDlpjDkFLAKGXl6oqq4SEXq1COPJ38WTeuQMN76+ghPZ2rJXqq6wJ9E3Ag6V2U6zlV1otIhsEpE5IhJzKeeKyB0ikiwiyenp6XaGruqaiT1i+eruXuQXlTDopWU8PT+V3SeyHR2WUle86roZOx+INcYkYLXaP7iUk40xbxpjEo0xieHh4dUUknKEZuF+fHpnD/rEhTFj1QGufmkZY15fyWfJh8g6V+jo8JS6ItkzH/1hIKbMdrStrJQxJrPM5tvAP8qc2++Cc5dcapCqfunSJJguTYLJyMlnzro0Zq05yENzNvGXL7fQr1U4v+vQkIFtIvDx0OUQlKoNUtlj7CLiBuwEBmIl7rXAeGNMapljoowxR23vRwGPGGO6227GrgM62w5dD3Qxxlz0KZvExESTnJxchSqpusYYQ8qh08zfeJSvNx3hRHY+nm4u9G4RxtXxkQxsHUFEgJejw1SqXhORdcaYxPL2VdqkMsYUicg9wALAFXjXGJMqItOBZGPMPOA+ERkBFAEngcm2c0+KyDNYXw4A0ytK8so5iQidGgfTqXEwfxnehjX7TrIg9RiLtx3n++3WNAodogN5ZFhrejYPc3C0SjmfSlv0tU1b9FcOYww7j+eweNtx3vt5P+6uwg9/7oe3h6ujQ1Oq3qmoRa9PxiqHERFaNfDn7v4t+M+EzhzNyuPRLzaRV6hP2CpVnTTRqzqha9MQxndrzFcpR/j7t9scHY5STkWHPag642+j2uMi8NGqA4xJiqFtw0BHh6SUU9AWvapTHhrcmmAfD578KpWSkrp1/0ip+koTvapTAn3ceXRYa9YdOMXn69McHY5STkETvapzRneOpkuTYJ7/bjvrDpzUJQuVqiJN9KrOcXERpo9sS25BMaP/u5Kn5291dEhK1Wua6FWd1LZhICseHcDYpBjeX7GfZTvTtWWv1GXSUTeqzgr29eCJa+NZviuDie+uITLAk6vbRPLk7+LxdNOHqpSylyZ6Vaf5erox9+5eLNx6jJ93ZzBz9UG2H8vmPxM6E6nz4yhlF+26UXVeuL8nE7o14T8TuvCnQS1Zd+AUf569UYdfKmUnTfSqXrlvYBzPXteOn3ZnMGP1AUeHo1S9oIle1TsTujXmqpbh/O3bbWw5nOXocJSq8zTRq3pHRPjH6AQ83VwZ+drPfJVyWCdCU6oCmuhVvdQg0Iuv7u5FszBf7p+VQpsn/8fwV5azP+Oso0NTqs7R+ehVvXYmr5BlO9PZcSybd3/ahwHiIv1pEe5Hiwg/ejQPpWNMkKPDVKrGVTQfvSZ65TQ2HjrN5+vT2H0ih90ncjiRnY+LwCe3d6dbs1BHh6dUjarSUoJK1RcdYoLoUKb1npmTz42vr+S2D5N5ZmQ7ruvUyHHBKeVA2kevnFaonyf/vbkL4X6e/Gl2CpPeXcPqvZmODkupWqeJXjm1Vg38mX9vb25KaszO49lMfm8tE99dw+4TOY4OTalao3306opx5PQ5/rVoJ4u2HaewqIQWEX40DvWlebgv47s2JkKnVFD1mN6MVaqMbUfPMGPVAQ6ezGV/5lkOnTxHk1Af+reKoG/LcPq3jnB0iEpdMk30SlVgxe4Mnv1mGwcyz5JbWMzbExMZ2CbS0WEpdUk00Stlh3MFxdz4xgq2HD7DmMRo/jI8nkBvd0eHpZRdKkr0dt2MFZGhIrJDRHaLyKMVHDdaRIyIJNq2Y0XknIik2F6vX14VlKp53h6uvH9LV0Z2bMjs5DSufmkph07mOjospaqs0kQvIq7Aa8AwIB4YJyLx5RznD9wPrL5g1x5jTEfb665qiFmpGhPm58nLN3XkrYmJ5BcWc8v7azlxJs/RYSlVJfa06LsCu40xe40xBcAsYGQ5xz0DvADo/xWqXhMRBsVH8sbvEzmQeZbeL/zIwtRjjg5LqctmT6JvBBwqs51mKyslIp2BGGPMN+Wc31RENojIUhHpc/mhKlW7ejQP5fOpPWkR4ce9n2zgwc828vbyveQX6UyZqn6p8gNTIuICvAT8uZzdR4HGxphOwJ+Aj0UkoJxr3CEiySKSnJ6eXtWQlKo2CdFBzLitG33iwli2M51nv9nG3TM3kJGT7+jQlLKbPYn+MBBTZjvaVnaeP9AOWCIi+4HuwDwRSTTG5BtjMgGMMeuAPUDLCz/AGPOmMSbRGJMYHh5+eTVRqoaE+Hrw9qQk1vzlap64Np7F245z1T9+5J2f9lFUXOLo8JSqlD2Tmq0F4kSkKVaCHwuMP7/TGJMFhJ3fFpElwIPGmGQRCQdOGmOKRaQZEAfsrcb4lapVt/ZuSruGATw1L5Vnvt7KrDUHaRHhR1SgN0PaRuosmapOqjTRG2OKROQeYAHgCrxrjEkVkelAsjFmXgWnXwVMF5FCoAS4yxhzsjoCV8pRujUL5X9/vIpP1hzk281H2Xk8mx93nODDlftp2zCAKb2bMqRtA7zcXR0dqlKAPjClVLU4k1fIvxfv4qddGew4ng1Ap8ZB3DcgjqggL2JDfTXxqxqlT8YqVUvyCotZuPU4e07k8OayvZyzrWUbFejFLb1iiQzwonPjYGJCfBwcqXI2muiVcoDTuQXsST/LoZO5vLhwB2mnzgHg4+HKq+M60aN5KD4euvaPqh6a6JVyMGMM2flFHMzM5c+zN5Z27/h7uhEe4EmEvycR/l6E+3tyfedGtG0Y6OCIVX2jiV6pOuRMXiGLUo9zPDuPE2fySc/O50R2HunZ+RzNysNFhNd/34W+LXWosbKfrhmrVB0S4OXO6C7R5e47kZ3H5HfXcuv7a4mL9GdijyaM69q4liNUzkZb9ErVMWfyCnn1+12s2XeSjWlZ9GoRSlSgN6G+HjQI9OKGLtH4e+n0yerXtEWvVD0S4OXOX4bHU1RcwkuLdrJsVzp708+SebaAgqISZq05xC29Ygny8aBtwwAdwaMqpS16peoJYwzLd2Vw7ycbyDpXCIC7q5DYJIQAbze6NAnm1t7NcHURB0eqHEFvxirlRPKLisnIKeD4mTxmrDzAgZO5nMotYG/6WTo1DqJVpD+dGwczomNDfUjrCqKJXiknZ4zh4zUHef/n/aTn5HM6txB/LzeGtWtAkI8Hfp5u+Hm6EezrzuD4Bvh6aq+ts9FEr9QVpKTEsHDrMWasOsiO49lk5xWSV/jLLJtNw3wZkxhDz+ahdIgJclygqlppolfqCldYXMLZ/CI2pWXx+JebS5/SbRHhx+D4SPy83GgTFUD/VhEOjlRdLk30SqlSxhhO5RYyO/kQs5MPcSAzl+ISKw/0iQujfaNAhidEEe7vSbifJyJ6c7c+0ESvlLooYwznCot57+f9fLz6IEezzmHL+7SI8KNTTBDNI/y4vlMjIgK8HBusuihN9Eopu6WdymXNvpNk5OTzzaajHDuTx/Ez1tKJk3o0YWCbSAK93QnwdifQ251gH3dt9dcBmuiVUlWScug0s5MP8fHqg7/Z171ZCPcOiKN7s1Adw+9AmuiVUtUi7VQux7LyyDpXyJm8Qg6fOscby/aSnVdEuL81C2eXJsGM6NCQxNgQR4d7RdFEr5SqMadzC1i87QTLdqZz8mwBK/ZkUGKgd4swRnRoyOgu0drSrwWa6JVStebk2QI+Xn2AmasPcjQrj7YNAxjQOoLIAC8Gx0fqDd0aooleKVXrjDF8vekoLy3ayYHMs6UjeTo1DmJ4+yhu7t5Ep2ioRprolVIOVVJi2Hr0DAtTj7Fw63G2H8tGBHo1DyMmxJsQXw+uigunabgvEf7a4r8cmuiVUnXKyj2ZLEg9xoo9GZw8W8jJs/mlLf42UQEE+7gT4e/JhO5NaN8oUFv+dtD56JVSdUqP5qH0aB5aun06t4DV+06yOS2L1CNZZOcVsWRnOnNTjgDQMSaIq1qG06aBf+kY/phgHwJ9dAEWe2iLXilVJ+UWFPHNpqPszzzLkh3ppB4586v9nm4uDG8fRUyID92bhRLs605sqO8V2/qvcteNiAwF/g24Am8bY56/yHGjgTlAkjEm2Vb2GHArUAzcZ4xZUNFnaaJXSpUnPTufjJx8ss4VknWukB+2nWDRtuOcyi3gfBrz93SjUbA3MSE+3NglmqZhvjQP98PlChjeWaWuGxFxBV4DBgFpwFoRmWeM2XrBcf7A/cDqMmXxwFigLdAQWCwiLY0xxZdbGaXUlSnc35Nwf8/S7SFtG/ACkJGTz9YjZziVW8CqvZlk5BSw8dBpFm09DkCIrwfhfp60auBPqwb+dIwJIiE68Ipad9eePvquwG5jzF4AEZkFjAS2XnDcM8ALwENlykYCs4wx+cA+Edltu97KqgaulFIAYX6eXNUyHICRHRsBUFBUQvL+kxw8mcu6A6c4lVvI0p3pzNto9fl7ubtwVVw4SbEhRAV5ERvqS9uGAU47Z489ib4RcKjMdhrQrewBItIZiDHGfCMiD11w7qoLzm10mbEqpZRdPNxc6NkijJ7A2K6NS8uzzhWyZt9Jvt92nOW7Mlhoa/UDtG7gT5+4MK5uE0lSbIhTdfdUedSNiLgALwGTq3CNO4A7ABo3blzJ0UopdXkCvd0ZFB/JoPhIjDGkZ+dzKreQ5AMnmb32EB+sPMBby/cR5udBVKA3/VuFExfpj5+XGwFebvh5uuPv5Ya/lxu+Hm715svAnkR/GIgpsx1tKzvPH2gHLLH92dMAmCciI+w4FwBjzJvAm2DdjL2E+JVS6rKICBEBXkQEeNGqgT8TujUht6CIBanH+GlXJodO5fLqj7u52HgVEfDzcMPPy42GQd40D/clLsKf4QlRNAzyrt3KVKLSUTci4gbsBAZiJem1wHhjTOpFjl8CPGiMSRaRtsDHWP3yDYHvgbiKbsbqqBulVF1x6mwBGTn5nMkrIie/iOy8QnJs78/kFZGTZ5XtPJ7N0aw8TmRb8/YHeLnRMtKfXi3CaBjkRd+WEUQG1OxqXVUadWOMKRKRe4AFWMMr3zXGpIrIdCDZGDOvgnNTRWQ21o3bIuBuHXGjlKovgn09CPb1sPv4A5ln+W7LMY6cPseafSf59/e7Sve5uQhBPu60jPSndYMA+sSFERXkRYS/F0He7jXaDaQPTCmlVA0pLC5hX8ZZlu/KICMnn5M5BWw9eoYdx7MpKCopPc7NRQjz8ySpaQivjut0WZ+lUyAopZQDuLu60DLSn5aR/r8qP5tfxJbDWaTn5JOe/csrIsDzIleqGk30SilVy3w93ejWLLTyA6uJS619klJKKYfQRK+UUk5OE71SSjk5TfRKKeXkNNErpZST00SvlFJOThO9Uko5OU30Sinl5OrcFAgikg4cqMIlwoCMagqnrruS6gpXVn21rs6rpurbxBgTXt6OOpfoq0pEki8234OzuZLqCldWfbWuzssR9dWuG6WUcnKa6JVSysk5Y6J/09EB1KIrqa5wZdVX6+q8ar2+TtdHr5RS6tecsUWvlFKqDKdJ9CIyVER2iMhuEXnU0fFUBxF5V0ROiMiWMmUhIrJIRHbZ/g22lYuIvGKr/yYR6ey4yC+diMSIyI8islVEUkXkflu509VXRLxEZI2IbLTV9WlbeVMRWW2r06ci4mEr97Rt77btj3VoBS6TiLiKyAYR+dq27ZT1FZH9IrJZRFJEJNlW5tDfY6dI9CLiCrwGDAPigXEiEu/YqKrF+8DQC8oeBb43xsRhLbZ+/kttGBBne90B/LeWYqwuRcCfjTHxQHfgbtt/Q2esbz4wwBjTAegIDBWR7sALwL+MMS2AU8CttuNvBU7Zyv9lO64+uh/YVmbbmevb3xjTscwwSsf+Hhtj6v0L6AEsKLP9GPCYo+OqprrFAlvKbO8Aomzvo4AdtvdvAOPKO64+voCvgEHOXl/AB1gPdMN6iMbNVl76Ow0sAHrY3rvZjhNHx36J9YzGSnADgK8Bcdb6AvuBsAvKHPp77BQteqARcKjMdpqtzBlFGmOO2t4fAyJt753mZ2D7U70TsBonra+tGyMFOAEsAvYAp40xRbZDytantK62/VlA7a1DVz1eBh4Gzq+IHYrz1tcAC0VknYjcYStz6O+xrhlbjxljjIg41bApEfEDPgf+aIw5IyKl+5ypvsaYYqCjiAQBXwKtHRtRzRGRa4ETxph1ItLPweHUht7GmMMiEgEsEpHtZXc64vfYWVr0h4GYMtvRtjJndFxEogBs/56wldf7n4GIuGMl+ZnGmC9sxU5bXwBjzGngR6yuiyAROd/4Kluf0rra9gcCmbUbaZX0AkaIyH5gFlb3zb9x0voaYw7b/j2B9SXeFQf/HjtLol8LxNnu4nsAY4F5Do6ppswDJtneT8Lqyz5fPtF2F787kFXmT8U6T6ym+zvANmPMS2V2OV19RSTc1pJHRLyx7kVsw0r4N9gOu7Cu538GNwA/GFuHbn1gjHnMGBNtjInF+n/zB2PMBJywviLiKyL+598Dg4EtOPr32NE3LqrxBsg1wE6svs6/ODqeaqrTJ8BRoBCr7+5WrL7K74FdwGIgxHasYI082gNsBhIdHf8l1rU3Vt/mJiDF9rrGGesLJAAbbHXdAjxpK28GrAF2A58BnrZyL9v2btv+Zo6uQxXq3g/42lnra6vTRtsr9XwucvTvsT4Zq5RSTs5Zum6UUkpdhCZ6pZRycprolVLKyWmiV0opJ6eJXimlnJwmeqWUcnKa6JVSyslpoldKKSf3/wHTqDr/gXP4hgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([index for index in range(len(evaluation_results[\"Train\"][\"binary_logloss\"]))],\n",
    "         evaluation_results[\"Train\"][\"binary_logloss\"], label=\"Train binary_logloss\")\n",
    "plt.plot([index for index in range(len(evaluation_results[\"Validate\"][\"binary_logloss\"]))],\n",
    "         evaluation_results[\"Validate\"][\"binary_logloss\"], label=\"Validate binary_logloss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[17  6]\n",
      " [12 32]] \n",
      "Accuracy: 73.13 % \n",
      "Precision: 84.21 % \n",
      "Recall: 72.73 % \n",
      "F1: 78.05 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred),\n",
    "      \"\\nAccuracy:\", round(accuracy_score(y_test, y_pred) * 100, 2), \"%\",\n",
    "      \"\\nPrecision:\", round(precision_score(y_test, y_pred) * 100, 2), \"%\",\n",
    "      \"\\nRecall:\", round(recall_score(y_test, y_pred) * 100, 2), \"%\",      \n",
    "      \"\\nF1:\", round(f1_score(y_test, y_pred) * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "SEED=42\n",
    "#model parameters defined\n",
    "model = CatBoostClassifier(iterations=500,\n",
    "                          depth=4,\n",
    "                          loss_function='Logloss',\n",
    "                          custom_metric=['Recall','F1','Precision'],\n",
    "                          eval_metric='F1', \n",
    "                          cat_features=cat,\n",
    "                          scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1] ,\n",
    "                          thread_count=-1,\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose=True,\n",
    "                          random_seed=SEED,\n",
    "                          has_time=False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.023635\n",
      "0:\tlearn: 0.7843137\ttest: 0.7407407\tbest: 0.7407407 (0)\ttotal: 54.7ms\tremaining: 27.3s\n",
      "1:\tlearn: 0.7843137\ttest: 0.7407407\tbest: 0.7407407 (0)\ttotal: 55.4ms\tremaining: 13.8s\n",
      "2:\tlearn: 0.7843137\ttest: 0.7407407\tbest: 0.7407407 (0)\ttotal: 56.7ms\tremaining: 9.39s\n",
      "3:\tlearn: 0.7843137\ttest: 0.7407407\tbest: 0.7407407 (0)\ttotal: 57.8ms\tremaining: 7.16s\n",
      "4:\tlearn: 0.7843137\ttest: 0.7407407\tbest: 0.7407407 (0)\ttotal: 58.3ms\tremaining: 5.77s\n",
      "5:\tlearn: 0.7843137\ttest: 0.7407407\tbest: 0.7407407 (0)\ttotal: 59ms\tremaining: 4.86s\n",
      "6:\tlearn: 0.7843137\ttest: 0.7407407\tbest: 0.7407407 (0)\ttotal: 59.6ms\tremaining: 4.2s\n",
      "7:\tlearn: 0.7843137\ttest: 0.7407407\tbest: 0.7407407 (0)\ttotal: 60.3ms\tremaining: 3.71s\n",
      "8:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 61.2ms\tremaining: 3.34s\n",
      "9:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 61.9ms\tremaining: 3.03s\n",
      "10:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 62.8ms\tremaining: 2.79s\n",
      "11:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 63.7ms\tremaining: 2.59s\n",
      "12:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 64.6ms\tremaining: 2.42s\n",
      "13:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 65.2ms\tremaining: 2.26s\n",
      "14:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 65.9ms\tremaining: 2.13s\n",
      "15:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 66.7ms\tremaining: 2.02s\n",
      "16:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 67.4ms\tremaining: 1.92s\n",
      "17:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 67.9ms\tremaining: 1.82s\n",
      "18:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 68.5ms\tremaining: 1.74s\n",
      "19:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 69.1ms\tremaining: 1.66s\n",
      "20:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 69.9ms\tremaining: 1.59s\n",
      "21:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 70.4ms\tremaining: 1.53s\n",
      "22:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 71.1ms\tremaining: 1.47s\n",
      "23:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 71.7ms\tremaining: 1.42s\n",
      "24:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 72.4ms\tremaining: 1.38s\n",
      "25:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 73ms\tremaining: 1.33s\n",
      "26:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 73.8ms\tremaining: 1.29s\n",
      "27:\tlearn: 0.8518519\ttest: 0.7636364\tbest: 0.7636364 (8)\ttotal: 74.5ms\tremaining: 1.25s\n",
      "28:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 75.2ms\tremaining: 1.22s\n",
      "29:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 75.8ms\tremaining: 1.19s\n",
      "30:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 76.5ms\tremaining: 1.16s\n",
      "31:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 77.1ms\tremaining: 1.13s\n",
      "32:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 77.8ms\tremaining: 1.1s\n",
      "33:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 78.3ms\tremaining: 1.07s\n",
      "34:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 79.2ms\tremaining: 1.05s\n",
      "35:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 79.8ms\tremaining: 1.03s\n",
      "36:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 80.5ms\tremaining: 1.01s\n",
      "37:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 81.2ms\tremaining: 987ms\n",
      "38:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 81.8ms\tremaining: 967ms\n",
      "39:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 82.3ms\tremaining: 947ms\n",
      "40:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 83ms\tremaining: 929ms\n",
      "41:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 83.7ms\tremaining: 912ms\n",
      "42:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 84.3ms\tremaining: 896ms\n",
      "43:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 85ms\tremaining: 881ms\n",
      "44:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 85.5ms\tremaining: 865ms\n",
      "45:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 86.1ms\tremaining: 850ms\n",
      "46:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 86.7ms\tremaining: 836ms\n",
      "47:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 87.3ms\tremaining: 823ms\n",
      "48:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 87.9ms\tremaining: 809ms\n",
      "49:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 88.5ms\tremaining: 797ms\n",
      "50:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 89.1ms\tremaining: 784ms\n",
      "51:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 89.6ms\tremaining: 772ms\n",
      "52:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 90ms\tremaining: 759ms\n",
      "53:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 90.7ms\tremaining: 749ms\n",
      "54:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 91.2ms\tremaining: 738ms\n",
      "55:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 91.8ms\tremaining: 728ms\n",
      "56:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 92.4ms\tremaining: 718ms\n",
      "57:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 92.9ms\tremaining: 708ms\n",
      "58:\tlearn: 0.8518519\ttest: 0.7368421\tbest: 0.7636364 (8)\ttotal: 93.5ms\tremaining: 699ms\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.7636363636\n",
      "bestIteration = 8\n",
      "\n",
      "Shrink model to first 9 iterations.\n"
     ]
    }
   ],
   "source": [
    "# fitting model\n",
    "model.fit(Pool(data=X_train, \n",
    "               label=y_train, \n",
    "              cat_features=cat,\n",
    "              #weight=X_train.COL_YOU_WANT_TO_WEIGHT\n",
    "              ),\n",
    "          eval_set = (X_validation, y_validation),\n",
    "          logging_level='Verbose'\n",
    "         )\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# saving model\n",
    "# dump(model, 'catboost_model.joblib')  \n",
    "# dump(model.evals_result_, 'evaluation_results_catboost.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[20  3]\n",
      " [15 29]] \n",
      "Accuracy: 73.13 % \n",
      "Precision: 90.62 % \n",
      "Recall: 65.91 % \n",
      "F1: 76.32 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred),\n",
    "      \"\\nAccuracy:\", round(accuracy_score(y_test, y_pred) * 100, 2), \"%\",\n",
    "      \"\\nPrecision:\", round(precision_score(y_test, y_pred) * 100, 2), \"%\",\n",
    "      \"\\nRecall:\", round(recall_score(y_test, y_pred) * 100, 2), \"%\",      \n",
    "      \"\\nF1:\", round(f1_score(y_test, y_pred) * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  <font style = 'color:blue'> Tuning ML Models </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GridSearch / RandomizedSearch\n",
    "\n",
    "There are several ways to tune model parameters, they are listed below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GridSearchCV\n",
    "Searches a pre-defined feature space to find the best model. It will search every combination possible, therefore is exhaustive but slow.\n",
    "\n",
    "More information can be found under: \n",
    "- GridSearchCV: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = titanic[[\"Pclass\", \"Sex\", \"Age\", \"Embarked_Q\", \"Embarked_S\"]]\n",
    "y = titanic.Survived\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 5, 'n_estimators': 5}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAM_GRID = {'n_estimators': [1, 5, 10], 'max_features': [1,2,3,4,5]}\n",
    "\n",
    "gs = GridSearchCV(estimator = RandomForestRegressor(random_state=42), param_grid=PARAM_GRID)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RandomizedSearchCV\n",
    "\n",
    "Searches a pre-defined feature space based on how many iterations (`n_iter`) you ask for, e.g. if you set `n_iter=5` it will try 5 random combinations.\n",
    "\n",
    "More information can be found under: \n",
    "- RandomizedSearchCV: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 10, 'max_features': 4}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "PARAM_GRID = {'n_estimators': [1, 5, 10], 'max_features': [1,2,3,4,5]}\n",
    "\n",
    "rs = RandomizedSearchCV(estimator = RandomForestRegressor(random_state=42), param_distributions=PARAM_GRID, \n",
    "                        n_iter=5)\n",
    "\n",
    "rs.fit(X_train, y_train)\n",
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  <font style = 'color:blue'> Summary </font>\n",
    "\n",
    "**Which model is best?** \n",
    "The best classifier for a particular task is task-dependent. In many business cases, interpretability is more important than accuracy. So, decision trees may be preferred. In other cases, accuracy on unseen data might be paramount, in which case random forests would likely be better (since they typically overfit less). \n",
    "\n",
    "**Remember that every model is a tradeoff between bias and variance. Ensemble models attempt to reduce overfitting by reducing variance but increasing bias (as compared to decision trees)**. \n",
    "\n",
    "By making the model more stable, we necessarily make it fit the training data less accurately. In some cases this is desired (particularly if we start with lots of overfitting), but for more simply structured data a simple decision tree might be best.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
